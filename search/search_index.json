{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lance Open Source Documentation!","text":"<p>Lance is a modern columnar data format optimized for machine learning and AI applications. It efficiently handles diverse multimodal data types while providing high-performance querying and versioning capabilities.</p> <p>Quickstart Locally With Python Read the Format Specification Train Your LLM on a Lance Dataset </p>"},{"location":"#how-does-lance-work","title":"\ud83c\udfaf How Does Lance Work?","text":"<p>Lance is designed to be used with images, videos, 3D point clouds, audio and tabular data. It supports any POSIX file systems, and cloud storage like AWS S3 and Google Cloud Storage.</p> <p>This file format is particularly suited for vector search, full-text search and LLM training on multimodal data. To learn more about how Lance works, read the format specification. </p> <p>Looking for LanceDB?</p> <p>This is the Lance table format project - the open source core that powers LanceDB. If you want the complete vector database and multimodal lakehouse built on Lance, visit lancedb.com</p>"},{"location":"#key-features-of-lance-format","title":"\u26a1 Key Features of Lance Format","text":"Feature Description \ud83d\ude80 High-Performance Random Access 100x faster than Parquet for random access patterns \ud83d\udd04 Zero-Copy Data Evolution Add, drop or update column data without rewriting the entire dataset \ud83c\udfa8 Multimodal Data Natively store large text, images, videos, documents and embeddings \ud83d\udd0d Vector Search Find nearest neighbors in under 1 millisecond with IVF-PQ, IVF-SQ, HNSW \ud83d\udcdd Full-Text Search Fast search over text with inverted index, Ngram index plus tokenizers \ud83d\udcbe Row Level Transaction Fully ACID transaction with row level conflict resolution"},{"location":"sdk_docs/","title":"SDK Documentation","text":"<p>Lance provides comprehensive documentation for all the language SDKs.  These auto-generated docs contain detailed information about  all classes, functions, and methods available in each language.</p> <ul> <li>Python</li> <li>Rust</li> <li>Java</li> </ul>"},{"location":"community/","title":"Guide for New Contributors","text":"<p>This is a guide for new contributors to the Lance project. Even if you have no previous experience with python, rust, and open source, you can still make an non-trivial impact by helping us improve documentation, examples, and more. For experienced developers, the issues you can work on run the gamut from warm-ups to serious challenges in python and rust.</p> <p>If you have any questions, please join our Discord for real-time support. Your feedback is always welcome!</p>"},{"location":"community/#getting-started","title":"Getting Started","text":"<ol> <li>Join our Discord and say hi</li> <li>Setup your development environment</li> <li>Pick an issue to work on. See https://github.com/lancedb/lance/contribute for good first issues.</li> <li>Have fun!</li> </ol>"},{"location":"community/#development-environment","title":"Development Environment","text":"<p>Currently Lance is implemented in Rust and comes with a Python wrapper. So you'll want to make sure you setup both.</p> <ol> <li>Install Rust: https://www.rust-lang.org/tools/install</li> <li>Install Python 3.9+: https://www.python.org/downloads/</li> <li>Install protoctol buffers: https://grpc.io/docs/protoc-installation/ (make sure you have version 3.20 or higher)</li> <li>Install commit hooks:     a. Install pre-commit: https://pre-commit.com/#install     b. Run <code>pre-commit install</code> in the root of the repo</li> </ol>"},{"location":"community/#sample-workflow","title":"Sample Workflow","text":"<ol> <li>Fork the repo</li> <li>Pick Github issue</li> <li>Create a branch for the issue</li> <li>Make your changes</li> <li>Create a pull request from your fork to lancedb/lance</li> <li>Get feedback and iterate</li> <li>Merge!</li> <li>Go back to step 2</li> </ol>"},{"location":"community/#python-development","title":"Python Development","text":"<p>The python integration is done via pyo3 + custom python code:</p> <ol> <li>The Rust code that directly supports the Python bindings are under <code>python/src</code> while the pure Python code lives under <code>python/python</code>.</li> <li>We make wrapper classes in Rust for Dataset/Scanner/RecordBatchReader that's exposed to python.</li> <li>These are then used by LanceDataset / LanceScanner implementations that extend pyarrow Dataset/Scanner for duckdb compat.</li> <li>Data is delivered via the Arrow C Data Interface</li> </ol> <p>To build the Python bindings, first install requirements:</p> <pre><code>pip install maturin\n</code></pre> <p>To make a dev install:</p> <pre><code>cd python\nmaturin develop\n</code></pre> <p>After installing, you can run <code>import lance</code> in a Python shell within the virtual environment.</p> <p>To run tests and integration tests: <pre><code>make test\nmake integtest\n</code></pre></p> <p>To run the tests on OS X, you may need to increase the default limit on the number of open files: <code>ulimit -n 2048</code></p>"},{"location":"community/#rust-development","title":"Rust Development","text":"<p>To format and lint Rust code:</p> <pre><code>cargo fmt --all\ncargo clippy --all-features --tests --benches\n</code></pre>"},{"location":"community/#core-format","title":"Core Format","text":"<p>The core format is implemented in Rust under the <code>rust</code> directory. Once you've setup Rust you can build the core format with:</p> <pre><code>cargo build\n</code></pre> <p>This builds the debug build. For the optimized release build:</p> <pre><code>cargo build -r\n</code></pre> <p>To run the Rust unit tests:</p> <pre><code>cargo test\n</code></pre> <p>If you're working on a performance related feature, benchmarks can be run via:</p> <pre><code>cargo bench\n</code></pre>"},{"location":"community/#documentation","title":"Documentation","text":""},{"location":"community/#main-website","title":"Main website","text":"<p>The main documentation website is built using mkdocs-material. To build the docs, first install requirements:</p> <pre><code>pip install -r docs/requirements.txt\n</code></pre> <p>Then build and start the docs server:</p> <pre><code>cd docs\nmkdocs serve\n</code></pre>"},{"location":"community/#python-generated-doc","title":"Python Generated Doc","text":"<p>Python code documentation is built using Sphinx in lance-python-doc, and published through Github Pages in ReadTheDocs style.</p>"},{"location":"community/#rust-generated-doc","title":"Rust Generated Doc","text":"<p>Rust code documentation is built and published to the Rust official docs website as a part of the release process.</p>"},{"location":"community/#example-notebooks","title":"Example Notebooks","text":"<p>Example notebooks are under <code>examples</code>.  These are standalone notebooks you should be able to download and run.</p>"},{"location":"community/#benchmarks","title":"Benchmarks","text":"<p>Our Rust benchmarks are run multiple times a day and the history can be found here.</p> <p>Separately, we have vector index benchmarks that test against the sift1m dataset, as well as benchmarks for tpch. These live under <code>benchmarks</code>.</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>We follow the Code of Conduct of Python Foundation and  Rust Foundation. </p>"},{"location":"community/lancelot/","title":"Lancelot: The Greatest Knight of the Open Source Round Table","text":"<p>Lancelot is the embodiment of bravery, loyalty, and a relentless pursuit of knowledge in the realm of open source.  As the Guardian of Open Source, a Lancelot champions developers everywhere,  wielding their lance as a symbol of empowerment and collaboration within the tech community.</p> <p>With a passion for innovation and technical prowess, Lancelot navigates the world of open source with grace.  They stand ready to assist fellow developers in their quests\u2014whether through sharing knowledge, contributing code,  or advocating for the transformative power of open source software. Lancelot encourages you to join them in building  a vibrant community around Lance and LanceDB Open Source.</p>"},{"location":"community/lancelot/#join-the-lancelot-round-table-champion-open-source-innovation","title":"Join the Lancelot Round Table: Champion Open Source Innovation","text":""},{"location":"community/lancelot/#step-1-enter-the-realm-of-contribution","title":"Step 1: Enter the Realm of Contribution","text":"<ul> <li>Knights of Code: Forge your legacy by contributing code via PRs or tackling issues on GitHub, leaving your mark on the digital battlefield!</li> <li>Stewards of the Community Square: Rally your fellow developers in our Discord \u2013 be the guiding light for problem solvers and innovators alike!</li> <li>Organizers of Knowledge: Host enlightening tech discussions or special interest groups to share your insights and empower others on their journeys.</li> <li>Beta Testers of the Future: Become our bug-hunting hero by testing new features in Lance and LanceDB, shaping the future with your keen eye!</li> <li>Bards of Content: Share your personal Lance and LanceDB journey through blogs, tutorials, or demos that inspire others to get started.</li> <li>Heralds of Social Media: Spread the word about Lance and LanceDB across your favorite platforms. Let our voice be heard by all. Let the knight's battle cry echo through the land!</li> </ul>"},{"location":"community/lancelot/#step-2-take-up-your-lance-of-advocacy","title":"Step 2: Take Up Your Lance of Advocacy","text":"<ul> <li>Champions of the Stage: Are you prepared to take the field? Present Lance/LanceDB at meetups or conferences and share your knowledge with the realm. Remember, every great knight needs a great story to tell!</li> <li>Storytellers of Innovation: Weave compelling tales through in-depth articles, guides, or case studies that explore the magic of Lance and LanceDB\u2019s features and real-world applications. Every tale deserves an epic plot twist!</li> <li>Guardians of Code: Protect the integrity of our codebase by reviewing PRs and guiding fellow contributors through challenges.</li> <li>Allies of Core Maintainers: Collaborate closely with our core maintainers, supporting their efforts to enhance and sustain the Lance community. Together, we shall create a lake of innovation!</li> </ul>"},{"location":"community/lancelot/#step-3-become-a-legendary-lancelot","title":"Step 3: Become a Legendary Lancelot","text":"<ul> <li>Cavalry of Innovation: Lead a game-changing initiative or PR that elevates Lance/LanceDB to new heights. Your bravery could change the course of history\u2014no pressure!</li> <li>Sages of Expertise: Become the go-to Lance expert in your developer circles, sharing wisdom far and wide. After all, every sage needs their followers!</li> <li>Guardians of Production: Champion Lance in your production environment and guide your team\u2019s AI journey. Your leadership will light the path forward.</li> <li>Minstrels of the Realm: Rock the stage at major tech conferences, singing praises of Lance and sharing its magic. Who says knights can\u2019t have great rhythm?</li> <li>Mentors of Community: Host office hours to share your expertise and help others on their journey. Your guidance will be as valuable as Excalibur itself!</li> <li>Noble Nominees: Earn a coveted nomination from a Lance/LanceDB maintainer, solidifying your status as a true Lancelot. Wear that title with pride!</li> </ul>"},{"location":"community/lancelot/#why-join-the-round-table-of-lancelot","title":"Why Join the Round Table of Lancelot?","text":"<ul> <li>Hone Your Skills: Sharpen your expertise in next-generation AI infrastructure tools that empower innovative solutions\u2014because every knight needs their trusty sword.</li> <li>Forge Connections: Unite with a vibrant community of fellow knights, AI enthusiasts, and database champions.</li> <li>Earn Recognition: Stand tall among your peers as you showcase your open source prowess and contributions. Your name will echo through the halls of innovation!</li> <li>Shape the Future: Play a vital role in crafting the next generation of multimodal AI databases and infrastructure.</li> </ul>"},{"location":"community/lancelot/#lancelots-treasures-await","title":"Lancelot\u2019s Treasures Await!","text":"<ul> <li>Exclusive Insights: Gain access to sneak peeks of new features and our secret roadmap, guiding you on your quest.</li> <li>Swag of Valor: Receive exclusive Lancelot merchandise that proudly displays your allegiance to the cause.</li> <li>Collaborative Adventures: Work hand-in-hand with our core team, sharing knowledge and shaping the future together.</li> <li>Opportunities to Shine: Seize the chance to speak at LanceDB events, sharing your journey and inspiring others.</li> </ul> <p>Ready to wield your lance and embark on this epic quest? Open an issue on our GitHub and show us your lance on Discord.  Let\u2019s make some AI database magic together!</p>"},{"location":"community/lancelot/#lancelot-round-table","title":"Lancelot Round Table","text":"# Name Github Profile Affiliation 1 Prashanth Rao prrao87 K\u00f9zu 2 Rong Rong walterddr Character.AI 3 Noah Shpak noahshpak Character.AI 4 Giuseppe Battista giusedroid AWS 5 Kevin Shaffer-Morrison kevinshaffermorrison AWS 6 Jiacheng Yang jiachengdb Databricks 7 Ankit Vij ankitvij-db Databricks 8 Akela Drissner-Schmid akelad dltHub 9 Chongchen Chen chenkovsky MiraclePlus 10 Vino Yang yanghua Bytedance 11 Zhaowei Huang SaintBacchus Bytedance 12 Jeremy Leibs jleibs Rerun.io 13 Aman Kishore AmanKishore Harvey.AI 14 Matt Basta mattbasta RunwayML 15 Timothy Carambat timothycarambat Anything LLM 16 Ty Dunn TyDunn Continue 17 Pablo Delgado pablete Netflix 18 Sangwu Lee RE-N-Y Krea.AI 19 Nat Roth nrothGIT Character.AI"},{"location":"community/lancelot/#hall-of-heroes","title":"Hall of Heroes","text":"@chenkovsky @yanghua @SaintBacchus @connellPortrait @takaebato @HoKim98 @Jay-ju @imotai @renato2099 @niyue @FuPeiJiang @MaxPowerWasTaken @emmanuel-ferdman @fzowl @fzliu @umuthopeyildirim @stevensu1977 @gagan-bhullar-tech @kursataktas @erikml-db @alexwilcoxson-rel @o-alexandrov @do-me @rithikJha @jameswu1991 @akashsara @sayandipdutta @rjrobben @PrashantDixit0 @ankitvij-db @jiachengdb @dentiny @tonyf @mattbasta @bllchmbrs @antoniomdk @ousiax @rahuljo @philz @wilhelmjung @h0rv @dsgibbons @maxburke @broccoliSpicy @BitPhinix @inn-0 @MagnusS0 @nuvic @JoanFM @thomasjpfan @sidharthrajaram @forrestmckee @NickDarvey @heiher @joshua-auchincloss @josca42 @beinan @harsha-mangena @paulwalsh-sonrai @paulrinaldi @gsilvestrin @vipul-maheshwari @ascillitoe @lyang24 @vjc578db @andrew-pienso @vaifai @jeff1010322 @fecet @andrijazz @kemingy @ahaapple @jgugglberger @bclavie @Akagi201 @schorfma @samuelcolvin @msu-reevo @alex766 @TD-Sky @timsaucer @triandco @HubertY @luohao @pmeier @PhorstenkampFuzzy @aaazzam @guspan-tanadi @enoonan"},{"location":"examples/python/artifact_management/","title":"Deep Learning Artifact Management using Lance","text":"<p>Along with datasets, Lance file format can also be used for saving and versioning deep learning model weights.  In fact deep learning artifact management can be made more streamlined (compared to vanilla weight saving methods) using Lance file format for PyTorch model weights.</p> <p>In this example we will be demonstrating how you save, version and load a PyTorch model's weights using Lance. More specifically we will be loading a pre-trained ResNet model, saving it in Lance file format, loading it back to PyTorch and verifying if the weights are still indeed the same. We will also be demonstrating how you can version your model weights in a single lance dataset thanks to our Zero-copy, automatic versioning.</p> <p>Key Idea: When you save a model's weights (read: state dictionary) in PyTorch, weights are stored as key-value pairs in an <code>OrderedDict</code> with the keys representing the weight's name and the value representing the corresponding weight tensor. To emulate this as closely as possible, we will be saving the weights in three columns. The first column will have the name of the weight, the second will have the weight itself but flattened in a list and the third will have the original shape of the weights so they can be reconstructed for loading into a model.</p>"},{"location":"examples/python/artifact_management/#imports-and-setup","title":"Imports and Setup","text":"<p>We will start by importing and loading all the necessary modules.</p> <pre><code>import os\nimport shutil\nimport lance\nimport pyarrow as pa\nimport torch\nfrom collections import OrderedDict\n</code></pre> <p>We will also define a <code>GLOBAL_SCHEMA</code> that will dictate how the weights table will look like.</p> <pre><code>GLOBAL_SCHEMA = pa.schema(\n    [\n        pa.field(\"name\", pa.string()),\n        pa.field(\"value\", pa.list_(pa.float64(), -1)),\n        pa.field(\"shape\", pa.list_(pa.int64(), -1)), # Is a list with variable shape because weights can have any number of dims\n    ]\n)\n</code></pre> <p>As we covered earlier, the weights table will have three columns - one for storing the weight name, one for storing the flattened weight value and one for storing the original weight shape for loading them back.</p>"},{"location":"examples/python/artifact_management/#saving-and-versioning-models","title":"Saving and Versioning Models","text":"<p>First we will focus on the model saving part. Let's start by writing a utility function that will take a model's state dict, goes over each weight, flatten it and then return the weight name, flattened weight and weight's original shape in a pyarrow <code>RecordBatch</code>.</p> <pre><code>def _save_model_writer(state_dict):\n    \"\"\"Yields a RecordBatch for each parameter in the model state dict\"\"\"\n    for param_name, param in state_dict.items():\n        param_shape = list(param.size())\n        param_value = param.flatten().tolist()\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [param_name],\n                    pa.string(),\n                ),\n                pa.array(\n                    [param_value],\n                    pa.list_(pa.float64(), -1),\n                ),\n                pa.array(\n                    [param_shape],\n                    pa.list_(pa.int64(), -1),\n                ),\n            ],\n            [\"name\", \"value\", \"shape\"],\n        )\n</code></pre> <p>Now about versioning: Let's say you trained your model on some new data but don't want to overwrite your old checkpoint, you can now just save these newly trained model weights as a version in Lance weights dataset. This will allow you to load specific version of weights from one lance weight dataset instead of making separate folders for each model checkpoint to make.</p> <p>Let's write a function that handles the work for saving the model, whether with versions or without them.</p> <pre><code>def save_model(state_dict: OrderedDict, file_name: str, version=False):\n    \"\"\"Saves a PyTorch model in lance file format\n\n    Args:\n        state_dict (OrderedDict): Model state dict\n        file_name (str): Lance model name\n        version (bool): Whether to save as a new version or overwrite the existing versions,\n            if the lance file already exists\n    \"\"\"\n    # Create a reader\n    reader = pa.RecordBatchReader.from_batches(\n        GLOBAL_SCHEMA, _save_model_writer(state_dict)\n    )\n\n    if os.path.exists(file_name):\n        if version:\n            # If we want versioning, we use the overwrite mode to create a new version\n            lance.write_dataset(\n                reader, file_name, schema=GLOBAL_SCHEMA, mode=\"overwrite\"\n            )\n        else:\n            # If we don't want versioning, we delete the existing file and write a new one\n            shutil.rmtree(file_name)\n            lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n    else:\n        # If the file doesn't exist, we write a new one\n        lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n</code></pre> <p>The above function will take in the model state dict, the lance saved file name and the weights version. The function will start by making a <code>RecordBatchReader</code> using the global schema and the utility function we wrote above. If the weights lance dataset already exists in the directory, we will just save it as a new version (if versioning is enabled) or delete the old file and save the weights as new. Otherwise the weights saving will be done normally.</p>"},{"location":"examples/python/artifact_management/#loading-models","title":"Loading Models","text":"<p>Loading weights from a Lance weight dataset into a model is just the reverse of saving them. The key part is to reshape the flattened weights back to their original shape, which is easier thanks to the shape that you saved corresponding to the weights. We will divide this into three functions for better readability.</p> <p>The first function will be the <code>_load_weight</code> function which will take a \"weight\" retrieved from the Lance weight dataset and return the weight as a torch tensor in its original shape. The \"weight\" that we retrieve from the Lance weight dataset will be a dict with value corresponding to each column in form of a key.</p> <pre><code>def _load_weight(weight: dict) -&gt; torch.Tensor:\n    \"\"\"Converts a weight dict to a torch tensor\"\"\"\n    return torch.tensor(weight[\"value\"], dtype=torch.float64).reshape(weight[\"shape\"])\n</code></pre> <p>Optionally, you could also add an option to specify the datatype of the weights.</p> <p>The next function will be on loading all the weights from the lance weight dataset into a state dictionary, which is what PyTorch will expect when we load the weights into our model.</p> <pre><code>def _load_state_dict(file_name: str, version: int = 1, map_location=None) -&gt; OrderedDict:\n    \"\"\"Reads the model weights from lance file and returns a model state dict\n    If the model weights are too large, this function will fail with a memory error.\n\n    Args:\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n\n    Returns:\n        OrderedDict: Model state dict\n    \"\"\"\n    ds = lance.dataset(file_name, version=version)\n    weights = ds.take([x for x in range(ds.count_rows())]).to_pylist()\n    state_dict = OrderedDict()\n\n    for weight in weights:\n        state_dict[weight[\"name\"]] = _load_weight(weight).to(map_location)\n\n    return state_dict\n</code></pre> <p>The <code>load_state_dict</code> function will expect a lance weight dataset file name, a version and a device where the weights will be loaded into.  We essentially load all the weights from the lance weight dataset into our memory and iteratively convert them into weights using the utility function we wrote earlier and then put them on the device.</p> <p>One thing to note here is that this function will fail if the saved weights are larger than memory. For the sake of simplicity, we assume the weights to be loaded can fit in the memory and we don't have to deal with any sharding.</p> <p>Finally, we will write a higher level function is the only one we will call to load the weights.</p> <pre><code>def load_model(\n    model: torch.nn.Module, file_name: str, version: int = 1, map_location=None\n):\n    \"\"\"Loads the model weights from lance file and sets them to the model\n\n    Args:\n        model (torch.nn.Module): PyTorch model\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n    \"\"\"\n    state_dict = _load_state_dict(file_name, version=version, map_location=map_location)\n    model.load_state_dict(state_dict)\n</code></pre> <p>The <code>load_model</code> function will require the model, the lance weight dataset name, the version of weights to load in and the map location. This will just call the <code>_load_state_dict</code> utility to get the state dict and then load that state dict into the model.</p>"},{"location":"examples/python/artifact_management/#conclusion","title":"Conclusion","text":"<p>In conclusion, you only need to call the two functions: <code>save_model</code> and <code>load_model</code> to save and load the models respectively and as long as the weights can be fit in the memory and are in PyTorch, it should be fine.</p> <p>Although experimental, this approach defines a new way of doing deep learning artifact management. </p>"},{"location":"examples/python/clip_training/","title":"Training Multi-Modal models using a Lance dataset","text":"<p>In this example we will be training a CLIP model for natural image based search using a Lance image-text dataset.  In particular, we will be using the flickr_8k Lance dataset.</p> <p>The model architecture and part of the training code are adapted from Manan Goel's Implementing CLIP with PyTorch Lightning with necessary changes to for a minimal, lance-compatible training example.</p>"},{"location":"examples/python/clip_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Along with Lance, we will be needing PyTorch and timm for our CLIP model to train.</p> <pre><code>import cv2\nimport lance\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm\nfrom transformers import AutoModel, AutoTokenizer\n\nimport itertools\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\n</code></pre> <p>Now, we will define a Config class that will house all the hyper-parameters required for training.</p> <pre><code>class Config:\n    img_size = (128, 128)\n    bs = 32\n    head_lr = 1e-3\n    img_enc_lr = 1e-4\n    text_enc_lr = 1e-5\n    max_len = 18\n    img_embed_dim = 2048\n    text_embed_dim = 768\n    projection_dim = 256\n    temperature = 1.0\n    num_epochs = 2\n    img_encoder_model = 'resnet50'\n    text_encoder_model = 'bert-base-cased'\n</code></pre> <p>And also two utility functions that will help us load the images and texts from the lance dataset.  Remember, our Lance dataset has images, image names and all the captions for a given image. We only need the images and one of those captions.  For simplicity, when loading captions, we will be choosing the one that is the longest (with the rather naive assumption that it has more information about the image).</p> <pre><code>def load_image(ds, idx):\n    # Utility function to load an image at an index and convert it from bytes format to img format\n    raw_img = ds.take([idx], columns=['image']).to_pydict()\n    raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n    img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img\n\ndef load_caption(ds, idx):\n    # Utility function to load an image's caption. Currently we return the longest caption of all\n    captions = ds.take([idx], columns=['captions']).to_pydict()['captions'][0]\n    return max(captions, key=len)\n</code></pre> <p>Since the images are stored as bytes in the lance dataset, the <code>load_image()</code> function will load the bytes corresponding to an image and then use numpy and opencv to convert it into an image.</p>"},{"location":"examples/python/clip_training/#dataset-and-augmentations","title":"Dataset and Augmentations","text":"<p>Since our CLIP model will expect images of same size and tokenized captions, we will define a custom PyTorch dataset that will take the lance dataset path along with any augmentation (for the image) and return a pre-processed image and a tokenized caption (as a dictionary).</p> <pre><code>class CLIPLanceDataset(Dataset):\n    \"\"\"Custom Dataset to load images and their corresponding captions\"\"\"\n    def __init__(self, lance_path, max_len=18, tokenizer=None, transforms=None):\n        self.ds = lance.dataset(lance_path)\n        self.max_len = max_len\n        # Init a new tokenizer if not specified already\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') if not tokenizer else tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.ds.count_rows()\n\n    def __getitem__(self, idx):\n        # Load the image and caption\n        img = load_image(self.ds, idx)\n        caption = load_caption(self.ds, idx)\n\n        # Apply transformations to the images\n        if self.transforms:\n            img = self.transforms(img)\n\n        # Tokenize the caption\n        caption = self.tokenizer(\n            caption,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        # Flatten each component of tokenized caption otherwise they will cause size mismatch errors during training\n        caption = {k: v.flatten() for k, v in caption.items()}\n\n        return img, caption\n</code></pre> <p>Now that our custom dataset is ready, we also define some very basic augmentations for our images.</p> <pre><code>train_augments = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Resize(Config.img_size),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n</code></pre> <p>The transformations are very basic: resizing all the images to be of the same shape and then normalizing them to stabilize the training later on.</p>"},{"location":"examples/python/clip_training/#model-and-setup","title":"Model and Setup","text":"<p>Since we our training a CLIP model, we have the following: * <code>ImageEncoder</code> that uses a pre-trained vision model (<code>resnet50</code> in this case) to convert images into feature vectors. * <code>TextEncoder</code> that uses a pre-trained language model (<code>bert-base-cased</code> in this case) to transform text captions into feature vectors. * <code>Head</code> which is a Projection module projects these feature vectors into a common embedding space.</p> <p>Going into deeper details of the CLIP model and its architectural nuances are out of the scope of this example, however if you wish to read more on it, you can read the official paper here.</p> <p>Now that we have understood the general summary of the model, let's define all the required modules.</p> <pre><code>class ImageEncoder(nn.Module):\n    \"\"\"Encodes the Image\"\"\"\n    def __init__(self, model_name, pretrained = True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"avg\"\n        )\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, img):\n        return self.backbone(img)\n\nclass TextEncoder(nn.Module):\n    \"\"\"Encodes the Caption\"\"\"\n    def __init__(self, model_name):\n        super().__init__()\n\n        self.backbone = AutoModel.from_pretrained(model_name)\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, captions):\n        output = self.backbone(**captions)\n        return output.last_hidden_state[:, 0, :]\n\nclass Head(nn.Module):\n    \"\"\"Projects both into Embedding space\"\"\"\n    def __init__(self, embedding_dim, projection_dim):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n\n        self.dropout = nn.Dropout(0.3)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x += projected\n\n        return self.layer_norm(x)\n</code></pre> <p>Along with the model definition, we will be defining two utility functions to simplify the training: <code>forward()</code> which will do one forward pass through the combined models and <code>loss_fn()</code> which will take the image and text embeddings output from <code>forward</code> function and then calculate the loss using them.</p> <pre><code>def loss_fn(img_embed, text_embed, temperature=0.2):\n    \"\"\"\n    https://arxiv.org/abs/2103.00020/\n    \"\"\"\n    # Calculate logits, image similarity and text similarity\n    logits = (text_embed @ img_embed.T) / temperature\n    img_sim = img_embed @ img_embed.T\n    text_sim = text_embed @ text_embed.T\n    # Calculate targets by taking the softmax of the similarities\n    targets = F.softmax(\n        (img_sim + text_sim) / 2 * temperature, dim=-1\n    )\n    img_loss = (-targets.T * nn.LogSoftmax(dim=-1)(logits.T)).sum(1)\n    text_loss = (-targets * nn.LogSoftmax(dim=-1)(logits)).sum(1)\n    return (img_loss + text_loss) / 2.0\n\ndef forward(img, caption):\n    # Transfer to device\n    img = img.to('cuda')\n    for k, v in caption.items():\n        caption[k] = v.to('cuda')\n\n    # Get embeddings for both img and caption\n    img_embed = img_head(img_encoder(img))\n    text_embed = text_head(text_encoder(caption))\n\n    return img_embed, text_embed\n</code></pre> <p>In order for us to train, we will define the models, tokenizer and the optimizer to be used in the next section</p> <pre><code># Define image encoder, image head, text encoder, text head and a tokenizer for tokenizing the caption\nimg_encoder = ImageEncoder(model_name=Config.img_encoder_model).to('cuda')\nimg_head = Head(Config.img_embed_dim, Config.projection_dim).to('cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(Config.text_encoder_model)\ntext_encoder = TextEncoder(model_name=Config.text_encoder_model).to('cuda')\ntext_head = Head(Config.text_embed_dim, Config.projection_dim).to('cuda')\n\n# Since we are optimizing two different models together, we will define parameters manually\nparameters = [\n    {\"params\": img_encoder.parameters(), \"lr\": Config.img_enc_lr},\n    {\"params\": text_encoder.parameters(), \"lr\": Config.text_enc_lr},\n    {\n        \"params\": itertools.chain(\n            img_head.parameters(),\n            text_head.parameters(),\n        ),\n        \"lr\": Config.head_lr,\n    },\n]\n\noptimizer = torch.optim.Adam(parameters)\n</code></pre>"},{"location":"examples/python/clip_training/#training","title":"Training","text":"<p>Before we actually train the model, one last step remains: which is to initialize our Lance dataset and a dataloader.</p> <pre><code># We assume the flickr8k.lance dataset is in the same directory\ndataset = CLIPLanceDataset(\n    lance_path=\"flickr8k.lance\",\n    max_len=Config.max_len,\n    tokenizer=tokenizer,\n    transforms=train_augments\n)\n\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=Config.bs,\n    pin_memory=True\n)\n</code></pre> <p>Now that our dataloader is initialized, let's train the model.</p> <pre><code>img_encoder.train()\nimg_head.train()\ntext_encoder.train()\ntext_head.train()\n\nfor epoch in range(Config.num_epochs):\n    print(f\"{'='*20} Epoch: {epoch+1} / {Config.num_epochs} {'='*20}\")\n\n    prog_bar = tqdm(dataloader)\n    for img, caption in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        img_embed, text_embed = forward(img, caption)\n        loss = loss_fn(img_embed, text_embed, temperature=Config.temperature).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n    print()\n</code></pre> <p>The training loop is quite self-explanatory. We set image encoder, image head, text encoder and text head models to training mode.  Then in each epoch, we iterate over our lance dataset, training the model and reporting the lance to the progress bar.</p> <pre><code>==================== Epoch: 1 / 2 ====================\nloss: 2.0799: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:14&lt;00:00,  1.88it/s]\n\n==================== Epoch: 2 / 2 ====================\nloss: 1.3064: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:10&lt;00:00,  1.94it/s]\n</code></pre> <p>And that's basically it! Using Lance dataset for training any type of model is very similar to using any other type of dataset but it also comes with increased speed and ease of use! </p>"},{"location":"examples/python/flickr8k_dataset_creation/","title":"Creating Multi-Modal datasets using Lance","text":"<p>Thanks to Lance file format's ability to store data of different modalities, one of the important use-cases that Lance shines in is storing Multi-modal datasets. In this brief example we will be going over how you can take a Multi-modal dataset and store it in Lance file format. </p> <p>The dataset of choice here is Flickr8k dataset. Flickr8k is a benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events.  The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations.</p> <p>We will be creating an Image-caption pair dataset for Multi-modal model training by using the above mentioned Flickr8k dataset, saving it in form of a Lance dataset with image file names, all captions for every image (order preserved) and the image itself (in binary format).</p>"},{"location":"examples/python/flickr8k_dataset_creation/#imports-and-setup","title":"Imports and Setup","text":"<p>We assume that you downloaded the dataset, more specifically the \"Flickr8k.token.txt\" file and the \"Flicker8k_Dataset/\" folder and both are present in the current directory. These can be downloaded from here (download both the dataset and text zip files).</p> <p>We also assume you have pyarrow and pylance installed as well as opencv (for reading in images) and tqdm (for pretty progress bars).</p> <p>Now let's start with imports and defining the caption file and image dataset folder.</p> <pre><code>import os\nimport cv2\nimport random\n\nimport lance\nimport pyarrow as pa\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\ncaptions = \"Flickr8k.token.txt\"\nimage_folder = \"Flicker8k_Dataset/\"\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#loading-and-processing","title":"Loading and Processing","text":"<p>In flickr8k dataset, each image has multiple corresponding captions that are ordered.  We are going to put all these captions in a list corresponding to each image with their position in the list representing the order in which they originally appear. Let's load the annotations (the image path and corresponding captions) in a list with each element of the list being a tuple consisting of image name, caption number and caption itself.</p> <pre><code>with open(captions, \"r\") as fl:\n    annotations = fl.readlines()\n\n# Converts the annotations where each element of this list is a tuple consisting of image file name, caption number and caption itself\nannotations = list(map(lambda x: tuple([*x.split('\\t')[0].split('#'), x.split('\\t')[1]]), annotations))\n</code></pre> <p>Now, for all captions of the same image, we will put them in a list sorted by their ordering.</p> <pre><code>captions = []\nimage_ids = set(ann[0] for ann in annotations)\nfor img_id in tqdm(image_ids):\n    current_img_captions = []\n    for ann_img_id, num, caption in annotations:\n        if img_id == ann_img_id:\n            current_img_captions.append((num, caption))\n\n    # Sort by the annotation number\n    current_img_captions.sort(key=lambda x: x[0])\n    captions.append((img_id, tuple([x[1] for x in current_img_captions])))\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#converting-to-a-lance-dataset","title":"Converting to a Lance Dataset","text":"<p>Now that our captions list is in a proper format, we will write a <code>process()</code> function that will take the said captions as argument and yield a Pyarrow record batch consisting of the <code>image_id</code>, <code>image</code> and <code>captions</code>. The image in this record batch will be in binary format and all the captions for an image will be in a list with their ordering preserved.</p> <pre><code>def process(captions):\n    for img_id, img_captions in tqdm(captions):\n        try:\n            with open(os.path.join(image_folder, img_id), 'rb') as im:\n                binary_im = im.read()\n\n        except FileNotFoundError:\n            print(f\"img_id '{img_id}' not found in the folder, skipping.\")\n            continue\n\n        img_id = pa.array([img_id], type=pa.string())\n        img = pa.array([binary_im], type=pa.binary())\n        capt = pa.array([img_captions], pa.list_(pa.string(), -1))\n\n        yield pa.RecordBatch.from_arrays(\n            [img_id, img, capt], \n            [\"image_id\", \"image\", \"captions\"]\n        )\n</code></pre> <p>Let's also define the same schema to tell Pyarrow the type of data it should be expecting in the table.</p> <pre><code>schema = pa.schema([\n    pa.field(\"image_id\", pa.string()),\n    pa.field(\"image\", pa.binary()),\n    pa.field(\"captions\", pa.list_(pa.string(), -1)),\n])\n</code></pre> <p>We are including the <code>image_id</code> (which is the original image name) so it can be easier to reference and debug in the future.</p> <p>Finally, we define a reader to iteratively read those record batches and then write them to a lance dataset on the disk.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(schema, process(captions))\nlance.write_dataset(reader, \"flickr8k.lance\", schema)\n</code></pre> <p>And that's basically it! If you want to execute this in a notebook form, you can check out this example in our deeplearning-recipes repository here.</p> <p>For more Deep learning related examples using Lance dataset, be sure to check out the lance-deeplearning-recipes repository! </p>"},{"location":"examples/python/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance","text":"<p>Lance can be used for creating and caching a text (or code) dataset for pre-training / fine-tuning of Large Language Models. The need for this arises when one needs to train a model on a subset of data or process the data in chunks without downloading all of it on the disk at once. This becomes a considerable problem when you just want a subset of a Terabyte or Petabyte-scale dataset.</p> <p>In this example, we will be bypassing this problem by downloading a text dataset in parts, tokenizing it and saving it as a Lance dataset.  This can be done for as many or as few data samples as you wish with average memory consumption approximately 3-4 GBs!</p> <p>For this example, we are working with the wikitext dataset, which is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.</p>"},{"location":"examples/python/llm_dataset_creation/#preparing-and-pre-processing-the-raw-dataset","title":"Preparing and pre-processing the raw dataset","text":"<p>Let's first define the dataset and the tokenizer</p> <pre><code>import lance\nimport pyarrow as pa\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom tqdm.auto import tqdm  # optional for progress tracking\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ndataset = load_dataset('wikitext', 'wikitext-103-raw-v1', streaming=True)['train']\ndataset = dataset.shuffle(seed=1337)\n</code></pre> <p>The <code>streaming</code> argument in <code>load_dataset</code> is especially important because if you run it without setting it to  <code>True</code>, the datasets library will download the entire dataset first, even though you only wish to use a subset of it. With <code>streaming</code> set to <code>True</code>, the samples will be downloaded as they are needed.</p> <p>Now we will define a function to help us with tokenizing our samples, one-by-one.</p> <pre><code>def tokenize(sample, field='text'):\n    return tokenizer(sample[field])['input_ids']\n</code></pre> <p>This function will receive a sample from a huggingface dataset and tokenize the values in the <code>field</code> column. This is the main text you want  to tokenize.</p>"},{"location":"examples/python/llm_dataset_creation/#creating-a-lance-dataset","title":"Creating a Lance dataset","text":"<p>Now that we have set up our raw dataset and pre-processing code,  let's define the main function that takes in the dataset, number of samples and field, and returns a pyarrow batch that will later be written into a lance dataset.</p> <pre><code>def process_samples(dataset, num_samples=100_000, field='text'):\n    current_sample = 0\n    for sample in tqdm(dataset, total=num_samples):\n        # If we have added all 5M samples, stop\n        if current_sample == num_samples:\n            break\n        if not sample[field]:\n            continue\n        # Tokenize the current sample\n        tokenized_sample = tokenize(sample, field)\n        # Increment the counter\n        current_sample += 1\n        # Yield a PyArrow RecordBatch\n        yield pa.RecordBatch.from_arrays(\n            [tokenized_sample], \n            names=[\"input_ids\"]\n        )\n</code></pre> <p>This function will be iterating over the huggingface dataset, one sample at a time, tokenizing the sample and yielding a pyarrow <code>RecordBatch</code> with all the tokens. We will do this until we have reached the <code>num_samples</code> number of samples or the end of the dataset, whichever comes first.</p> <p>Please note that by 'sample', we mean one example (row) in the original dataset. What one example exactly means will depend on the dataset itself as it could  be one line or an entire file of text. In this example, it varies in length between a line and a paragraph of text.</p> <p>We also need to define a schema to tell Lance what type of data we are expecting in our table. Since our dataset consists only of tokens which are long integers, <code>int64</code> is the suitable datatype.</p> <pre><code>schema = pa.schema([\n    pa.field(\"input_ids\", pa.int64())\n])\n</code></pre> <p>Finally, we need to define a <code>reader</code> that will be reading a stream of record batches from our <code>process_samples</code> function that yields  said record batches consisting of individual tokenized samples.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(\n    schema, \n    process_samples(dataset, num_samples=500_000, field='text') # For 500K samples\n)\n</code></pre> <p>And finally we use the <code>lance.write_dataset</code> which will write the dataset to the disk.</p> <pre><code># Write the dataset to disk\nlance.write_dataset(\n    reader, \n    \"wikitext_500K.lance\",\n    schema\n)\n</code></pre> <p>If you want to apply some other pre-processing to the tokens before saving it to the disk (like masking, etc), you may add it in the  <code>process_samples</code> function.</p> <p>And that's it! Your dataset has been tokenized and saved to the disk! </p>"},{"location":"examples/python/llm_training/","title":"Training LLMs using a Lance text dataset","text":"<p>Using a Lance text dataset for pre-training / fine-tuning a Large Language model is straightforward and memory-efficient. This example follows up on the Creating text dataset for LLM training using Lance example. Check it out if you haven't already.</p> <p>In this example, we will be training an LLM using \ud83e\udd17 transformers on the tokenized \"wikitext_500K\" lance dataset we created in the aforementioned example.</p>"},{"location":"examples/python/llm_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Let's setup our environment by doing all the necessary imports and defining a few basic things.</p> <pre><code>import numpy as np\nimport lance\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm.auto import tqdm\n\n# We'll be training the pre-trained GPT2 model in this example\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Also define some hyperparameters\nlr = 3e-4\nnb_epochs = 10\nblock_size = 1024\nbatch_size = 8\ndevice = 'cuda:0'\ndataset_path = 'wikitext_500K.lance'\n</code></pre> <p>Now that the basic setup is out of the way, let's define our custom Dataset and a Sampler for streaming the tokens from our Lance dataset.</p>"},{"location":"examples/python/llm_training/#data-loading-setup","title":"Data-loading Setup","text":"<p>We start by defining a utility function that will help us load any number of tokens from our lance dataset in a 'chunk'.</p> <pre><code>def from_indices(dataset, indices):\n    \"\"\"Load the elements on given indices from the dataset\"\"\"\n    chunk = dataset.take(indices).to_pylist()\n    chunk = list(map(lambda x: x['input_ids'], chunk))\n    return chunk\n</code></pre> <p>Now let's define our custom dataset and sampler for loading the tokens.</p> <pre><code>class LanceDataset(Dataset):\n    def __init__(\n        self,\n        dataset_path,\n        block_size,\n    ):\n        # Load the lance dataset from the saved path\n        self.ds = lance.dataset(dataset_path)\n        self.block_size = block_size\n\n        # Doing this so the sampler never asks for an index at the end of text\n        self.length = self.ds.count_rows() - block_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generate a window of indices starting from the current idx to idx+block_size\n        and return the tokens at those indices\n        \"\"\"\n        window = np.arange(idx, idx + self.block_size)\n        sample = from_indices(self.ds, window)\n\n        return {\"input_ids\": torch.tensor(sample), \"labels\": torch.tensor(sample)}\n</code></pre> <p>When given a random index by the sampler, the dataset will load the next <code>block_size</code> number of tokens starting from current index. This would in-essence form a sample as the loaded tokens would be causal.</p> <p>However we also need to make sure that the tokens we get from the dataset aren't overlapping. Let's understand this from an example:</p> <p>Let's say, for some arbitrary block size, during the training loop the dataset return the following tokens:</p> <p><code>\"Vienna is the capital of Austria\"</code> at index = 12 for sample #1, and,</p> <p><code>\"is the capital of Austria and\"</code> at index = 13 for sample #2, and so on</p> <p>The problem here is that if we allow the dataloader to fetch the 'samples' for any arbitrary number of indices, they may overlap (as we see above). This is not good for the model as it may start to overfit after seeing sufficient overlapping tokens.</p> <p>To solve this problem, we define a custom Sampler that only returns the indices that are 'block_size' apart from each other, ensuring that we don't see any overlapping samples.</p> <pre><code>class LanceSampler(Sampler):\n    r\"\"\"Samples tokens randomly but `block_size` indices apart.\n\n    Args:\n        data_source (Dataset): dataset to sample from\n        block_size (int): minimum index distance between each random sample\n    \"\"\"\n\n    def __init__(self, data_source, block_size=512):\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n        self.available_indices = list(range(0, self.num_samples, block_size))\n        np.random.shuffle(self.available_indices)\n\n    def __iter__(self):\n        yield from self.available_indices\n\n    def __len__(self) -&gt; int:\n        return len(self.available_indices)\n</code></pre> <p>Now when we fetch the tokens from our dataset with sampler being the <code>LanceSampler</code>, all samples in all the batches that our model sees during the training are guaranteed to be non-overlapping.</p> <p>This is done by generating a list of indices starting from 0 to the end of the dataset (which if you remember is lance dataset length - block size) with each index 'block_size' apart from the other. We then shuffle this list and yield indices from it.</p> <p>And that's basically it for the Dataloading! Now all we are left is to train the model!</p>"},{"location":"examples/python/llm_training/#model-training","title":"Model Training","text":"<p>Now you train the model just like you would with any other dataset!</p> <pre><code># Define the dataset, sampler and dataloader\ndataset = LanceDataset(dataset_path, block_size)\nsampler = LanceSampler(dataset, block_size)\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=batch_size,\n    sampler=sampler,\n    pin_memory=True\n)\n\n# Define the optimizer, training loop and train the model!\nmodel = model.to(device)\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\nfor epoch in range(nb_epochs):\n    print(f\"========= Epoch: {epoch+1} / {nb_epochs} =========\")\n    epoch_loss = []\n    prog_bar = tqdm(dataloader, total=len(dataloader))\n    for batch in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        # Put both input_ids and labels to the device\n        for k, v in batch.items():\n            batch[k] = v.to(device)\n\n        # Perform one forward pass and get the loss\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        # Perform backward pass\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n\n        epoch_loss.append(loss.item())\n\n    # Calculate training perplexity for this epoch\n    try:\n        perplexity = np.exp(np.mean(epoch_loss))\n    except OverflowError:\n        perplexity = float(\"-inf\")\n\n    print(f\"train_perplexity: {perplexity}\")\n</code></pre> <p>One tip: If your lance dataset is huge (like the wikitext_500K is), and you want to debug the model to look out for errors, you may want to wrap the dataloader in an <code>iter()</code> function and only run it for a couple batches.</p> <p>And that's basically it!</p> <p>The best part about using Lance, the custom Dataset and Sampler is that you get a whopping 95% average GPU utilisation and minimal CPU overhead thanks to the lightning fast random access that Lance provides \ud83d\ude80 </p>"},{"location":"examples/rust/hnsw/","title":"Indexing a dataset with HNSW (Hierarchical Navigable Small World)","text":"<p>HNSW is a graph based algorithm for approximate neighbor search in high-dimensional spaces. In this example, we will demonstrate how to build an HNSW vector index against a Lance dataset.</p> <p>This example will show how to:</p> <ol> <li>Generate synthetic test data of specified dimensions</li> <li>Build a hierarchical graph structure for efficient vector search using Lance API</li> <li>Perform vector search with different parameters and compute the ground truth using L2 distance search</li> </ol>"},{"location":"examples/rust/hnsw/#complete-example","title":"Complete Example","text":"<pre><code>use std::collections::HashSet;\nuse std::sync::Arc;\n\nuse arrow::array::{types::Float32Type, Array, FixedSizeListArray};\nuse arrow::array::{AsArray, FixedSizeListBuilder, Float32Builder};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchIterator;\nuse arrow_select::concat::concat;\nuse futures::stream::StreamExt;\nuse lance::Dataset;\nuse lance_index::vector::v3::subindex::IvfSubIndex;\nuse lance_index::vector::{\n    flat::storage::FlatFloatStorage,\n    hnsw::{builder::HnswBuildParams, HNSW},\n};\nuse lance_linalg::distance::DistanceType;\n\nfn ground_truth(fsl: &amp;FixedSizeListArray, query: &amp;[f32], k: usize) -&gt; HashSet&lt;u32&gt; {\n    let mut dists = vec![];\n    for i in 0..fsl.len() {\n        let dist = lance_linalg::distance::l2_distance(\n            query,\n            fsl.value(i).as_primitive::&lt;Float32Type&gt;().values(),\n        );\n        dists.push((dist, i as u32));\n    }\n    dists.sort_by(|a, b| a.0.partial_cmp(&amp;b.0).unwrap());\n    dists.truncate(k);\n    dists.into_iter().map(|(_, i)| i).collect()\n}\n\npub async fn create_test_vector_dataset(output: &amp;str, num_rows: usize, dim: i32) {\n    let schema = Arc::new(Schema::new(vec![Field::new(\n        \"vector\",\n        DataType::FixedSizeList(Arc::new(Field::new(\"item\", DataType::Float32, true)), dim),\n        false,\n    )]));\n\n    let mut batches = Vec::new();\n\n    // Create a few batches\n    for _ in 0..2 {\n        let v_builder = Float32Builder::new();\n        let mut list_builder = FixedSizeListBuilder::new(v_builder, dim);\n\n        for _ in 0..num_rows {\n            for _ in 0..dim {\n                list_builder.values().append_value(rand::random::&lt;f32&gt;());\n            }\n            list_builder.append(true);\n        }\n        let array = Arc::new(list_builder.finish());\n        let batch = RecordBatch::try_new(schema.clone(), vec![array]).unwrap();\n        batches.push(batch);\n    }\n    let batch_reader = RecordBatchIterator::new(batches.into_iter().map(Ok), schema.clone());\n    println!(\"Writing dataset to {}\", output);\n    Dataset::write(batch_reader, output, None).await.unwrap();\n}\n\n#[tokio::main]\nasync fn main() {\n    let uri: Option&lt;String&gt; = None; // None means generate test data\n    let column = \"vector\";\n    let ef = 100;\n    let max_edges = 30;\n    let max_level = 7;\n\n    // 1. Generate a synthetic test data of specified dimensions\n    let dataset = if uri.is_none() {\n        println!(\"No uri is provided, generating test dataset...\");\n        let output = \"test_vectors.lance\";\n        create_test_vector_dataset(output, 1000, 64).await;\n        Dataset::open(output).await.expect(\"Failed to open dataset\")\n    } else {\n        Dataset::open(uri.as_ref().unwrap())\n            .await\n            .expect(\"Failed to open dataset\")\n    };\n\n    println!(\"Dataset schema: {:#?}\", dataset.schema());\n    let batches = dataset\n        .scan()\n        .project(&amp;[column])\n        .unwrap()\n        .try_into_stream()\n        .await\n        .unwrap()\n        .then(|batch| async move { batch.unwrap().column_by_name(column).unwrap().clone() })\n        .collect::&lt;Vec&lt;_&gt;&gt;()\n        .await;\n    let arrs = batches.iter().map(|b| b.as_ref()).collect::&lt;Vec&lt;_&gt;&gt;();\n    let fsl = concat(&amp;arrs).unwrap().as_fixed_size_list().clone();\n    println!(\"Loaded {:?} batches\", fsl.len());\n\n    let vector_store = Arc::new(FlatFloatStorage::new(fsl.clone(), DistanceType::L2));\n\n    let q = fsl.value(0);\n    let k = 10;\n    let gt = ground_truth(&amp;fsl, q.as_primitive::&lt;Float32Type&gt;().values(), k);\n\n    for ef_construction in [15, 30, 50] {\n        let now = std::time::Instant::now();\n        // 2. Build a hierarchical graph structure for efficient vector search using Lance API\n        let hnsw = HNSW::index_vectors(\n            vector_store.as_ref(),\n            HnswBuildParams::default()\n                .max_level(max_level)\n                .num_edges(max_edges)\n                .ef_construction(ef_construction),\n        )\n        .unwrap();\n        let construct_time = now.elapsed().as_secs_f32();\n        let now = std::time::Instant::now();\n        // 3. Perform vector search with different parameters and compute the ground truth using L2 distance search\n        let results: HashSet&lt;u32&gt; = hnsw\n            .search_basic(q.clone(), k, ef, None, vector_store.as_ref())\n            .unwrap()\n            .iter()\n            .map(|node| node.id)\n            .collect();\n        let search_time = now.elapsed().as_micros();\n        println!(\n            \"level={}, ef_construct={}, ef={} recall={}: construct={:.3}s search={:.3} us\",\n            max_level,\n            ef_construction,\n            ef,\n            results.intersection(&amp;gt).count() as f32 / k as f32,\n            construct_time,\n            search_time\n        );\n    }\n}\n</code></pre>"},{"location":"examples/rust/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance in Rust","text":"<p>In this example, we will demonstrate how to achieve the Python example - LLM dataset creation shown in the Python examples in Rust.</p> <p>Note</p> <p>The huggingface Python API supports loading data in streaming mode and shuffling is provided as a builtin feature. Rust API lacks these feature thus the data are manually downloaded and shuffled within each batch.</p> <p>This example will show how to:</p> <ol> <li>Download and process a text dataset in parts from huggingface</li> <li>Tokenize the text data with a custom RecordBatchReader</li> <li>Save it as a Lance dataset using Lance API</li> </ol> <p>The implementation details in Rust will follow similar concepts as the Python version, but with Rust-specific APIs and patterns which are significantly more verbose.</p>"},{"location":"examples/rust/llm_dataset_creation/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::{Array, Int64Builder, ListBuilder, UInt32Array};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchReader;\nuse futures::StreamExt;\nuse hf_hub::{api::sync::Api, Repo, RepoType};\nuse lance::dataset::WriteParams;\nuse parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;\nuse rand::seq::SliceRandom;\nuse rand::SeedableRng;\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::Write;\nuse std::sync::Arc;\nuse tempfile::NamedTempFile;\nuse tokenizers::Tokenizer;\n\n// Implement a custom stream batch reader\nstruct WikiTextBatchReader {\n    schema: Arc&lt;Schema&gt;,\n    parquet_readers: Vec&lt;Option&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;&gt;,\n    current_reader_idx: usize,\n    current_reader: Option&lt;Box&lt;dyn RecordBatchReader + Send&gt;&gt;,\n    tokenizer: Tokenizer,\n    num_samples: u64,\n    cur_samples_cnt: u64,\n}\n\nimpl WikiTextBatchReader {\n    fn new(\n        parquet_readers: Vec&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;,\n        tokenizer: Tokenizer,\n        num_samples: Option&lt;u64&gt;,\n    ) -&gt; Result&lt;Self, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n        let schema = Arc::new(Schema::new(vec![Field::new(\n            \"input_ids\",\n            DataType::List(Arc::new(Field::new(\"item\", DataType::Int64, true))),\n            false,\n        )]));\n\n        Ok(Self {\n            schema,\n            parquet_readers: parquet_readers.into_iter().map(Some).collect(),\n            current_reader_idx: 0,\n            current_reader: None,\n            tokenizer,\n            num_samples: num_samples.unwrap_or(100_000),\n            cur_samples_cnt: 0,\n        })\n    }\n\n    fn process_batch(\n        &amp;mut self,\n        input_batch: &amp;RecordBatch,\n    ) -&gt; Result&lt;RecordBatch, arrow::error::ArrowError&gt; {\n        let num_rows = input_batch.num_rows();\n        let mut token_builder = ListBuilder::new(Int64Builder::with_capacity(num_rows * 1024)); // Pre-allocate space\n        let mut should_break = false;\n\n        let column = input_batch.column_by_name(\"text\").unwrap();\n        let string_array = column\n            .as_any()\n            .downcast_ref::&lt;arrow::array::StringArray&gt;()\n            .unwrap();\n        for i in 0..num_rows {\n            if self.cur_samples_cnt &gt;= self.num_samples {\n                should_break = true;\n                break;\n            }\n            if !Array::is_null(string_array, i) {\n                let text = string_array.value(i);\n                // Split paragraph into lines\n                for line in text.split('\\n') {\n                    if let Ok(encoding) = self.tokenizer.encode(line, true) {\n                        let tb_values = token_builder.values();\n                        for &amp;id in encoding.get_ids() {\n                            tb_values.append_value(id as i64);\n                        }\n                        token_builder.append(true);\n                        self.cur_samples_cnt += 1;\n                        if self.cur_samples_cnt % 5000 == 0 {\n                            println!(\"Processed {} rows\", self.cur_samples_cnt);\n                        }\n                        if self.cur_samples_cnt &gt;= self.num_samples {\n                            should_break = true;\n                            break;\n                        }\n                    }\n                }\n            }\n        }\n\n        // Create array and shuffle it\n        let input_ids_array = token_builder.finish();\n\n        // Create shuffled array by randomly sampling indices\n        let mut rng = rand::rngs::StdRng::seed_from_u64(1337);\n        let len = input_ids_array.len();\n        let mut indices: Vec&lt;u32&gt; = (0..len as u32).collect();\n        indices.shuffle(&amp;mut rng);\n\n        // Take values in shuffled order\n        let indices_array = UInt32Array::from(indices);\n        let shuffled = arrow::compute::take(&amp;input_ids_array, &amp;indices_array, None)?;\n\n        let batch = RecordBatch::try_new(self.schema.clone(), vec![Arc::new(shuffled)]);\n        if should_break {\n            println!(\"Stop at {} rows\", self.cur_samples_cnt);\n            self.parquet_readers.clear();\n            self.current_reader = None;\n        }\n\n        batch\n    }\n}\n\nimpl RecordBatchReader for WikiTextBatchReader {\n    fn schema(&amp;self) -&gt; Arc&lt;Schema&gt; {\n        self.schema.clone()\n    }\n}\n\nimpl Iterator for WikiTextBatchReader {\n    type Item = Result&lt;RecordBatch, arrow::error::ArrowError&gt;;\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        loop {\n            // If we have a current reader, try to get next batch\n            if let Some(reader) = &amp;mut self.current_reader {\n                if let Some(batch_result) = reader.next() {\n                    return Some(batch_result.and_then(|batch| self.process_batch(&amp;batch)));\n                }\n            }\n\n            // If no current reader or current reader is exhausted, try to get next reader\n            if self.current_reader_idx &lt; self.parquet_readers.len() {\n                if let Some(builder) = self.parquet_readers[self.current_reader_idx].take() {\n                    match builder.build() {\n                        Ok(reader) =&gt; {\n                            self.current_reader = Some(Box::new(reader));\n                            self.current_reader_idx += 1;\n                            continue;\n                        }\n                        Err(e) =&gt; {\n                            return Some(Err(arrow::error::ArrowError::ExternalError(Box::new(e))))\n                        }\n                    }\n                }\n            }\n\n            // No more readers available\n            return None;\n        }\n    }\n}\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let rt = tokio::runtime::Runtime::new()?;\n    rt.block_on(async {\n        // Load tokenizer\n        let tokenizer = load_tokenizer(\"gpt2\")?;\n\n        // Set up Hugging Face API\n        // Download from https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-103-raw-v1\n        let api = Api::new()?;\n        let repo = api.repo(Repo::with_revision(\n            \"Salesforce/wikitext\".into(),\n            RepoType::Dataset,\n            \"main\".into(),\n        ));\n\n        // Define the parquet files we want to download\n        let train_files = vec![\n            \"wikitext-103-raw-v1/train-00000-of-00002.parquet\",\n            \"wikitext-103-raw-v1/train-00001-of-00002.parquet\",\n        ];\n\n        let mut parquet_readers = Vec::new();\n        for file in &amp;train_files {\n            println!(\"Downloading file: {}\", file);\n            let file_path = repo.get(file)?;\n            let data = std::fs::read(file_path)?;\n\n            // Create a temporary file in the system temp directory and write the downloaded data to it\n            let mut temp_file = NamedTempFile::new()?;\n            temp_file.write_all(&amp;data)?;\n\n            // Create the parquet reader builder with a larger batch size\n            let builder = ParquetRecordBatchReaderBuilder::try_new(temp_file.into_file())?\n                .with_batch_size(8192); // Increase batch size for better performance\n            parquet_readers.push(builder);\n        }\n\n        if parquet_readers.is_empty() {\n            println!(\"No parquet files found to process.\");\n            return Ok(());\n        }\n\n        // Create batch reader\n        let num_samples: u64 = 500_000;\n        let batch_reader = WikiTextBatchReader::new(parquet_readers, tokenizer, Some(num_samples))?;\n\n        // Save as Lance dataset\n        println!(\"Writing to Lance dataset...\");\n        let lance_dataset_path = \"rust_wikitext_lance_dataset.lance\";\n\n        let write_params = WriteParams::default();\n        lance::Dataset::write(batch_reader, lance_dataset_path, Some(write_params)).await?;\n\n        // Verify the dataset\n        let ds = lance::Dataset::open(lance_dataset_path).await?;\n        let scanner = ds.scan();\n        let mut stream = scanner.try_into_stream().await?;\n\n        let mut total_rows = 0;\n        while let Some(batch_result) = stream.next().await {\n            let batch = batch_result?;\n            total_rows += batch.num_rows();\n        }\n\n        println!(\n            \"Lance dataset created successfully with {} rows\",\n            total_rows\n        );\n        println!(\"Dataset location: {}\", lance_dataset_path);\n\n        Ok(())\n    })\n}\n\nfn load_tokenizer(model_name: &amp;str) -&gt; Result&lt;Tokenizer, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let api = Api::new()?;\n    let repo = api.repo(Repo::with_revision(\n        model_name.into(),\n        RepoType::Model,\n        \"main\".into(),\n    ));\n\n    let tokenizer_path = repo.get(\"tokenizer.json\")?;\n    let tokenizer = Tokenizer::from_file(tokenizer_path)?;\n\n    Ok(tokenizer)\n}\n</code></pre>"},{"location":"examples/rust/write_read_dataset/","title":"Writing and reading a dataset using Lance","text":"<p>In this example, we will write a simple lance dataset to disk. Then we will read it and print out some basic properties like the schema and sizes for each record batch in the dataset. The example uses only one record batch, however it should work for larger datasets (multiple record batches) as well.</p>"},{"location":"examples/rust/write_read_dataset/#writing-the-raw-dataset","title":"Writing the raw dataset","text":"<pre><code>// Writes sample dataset to the given path\nasync fn write_dataset(data_path: &amp;str) {\n    // Define new schema\n    let schema = Arc::new(Schema::new(vec![\n        Field::new(\"key\", DataType::UInt32, false),\n        Field::new(\"value\", DataType::UInt32, false),\n    ]));\n\n    // Create new record batches\n    let batch = RecordBatch::try_new(\n        schema.clone(),\n        vec![\n            Arc::new(UInt32Array::from(vec![1, 2, 3, 4, 5, 6])),\n            Arc::new(UInt32Array::from(vec![6, 7, 8, 9, 10, 11])),\n        ],\n    )\n    .unwrap();\n\n    let batches = RecordBatchIterator::new([Ok(batch)], schema.clone());\n\n    // Define write parameters (e.g. overwrite dataset)\n    let write_params = WriteParams {\n        mode: WriteMode::Overwrite,\n        ..Default::default()\n    };\n\n    Dataset::write(batches, data_path, Some(write_params))\n        .await\n        .unwrap();\n} // End write dataset\n</code></pre> <p>First we define a schema for our dataset, and create a record batch from that schema. Next we iterate over the record batches (only one in this case) and write them to disk. We also define the write parameters (set to overwrite) and then write the dataset to disk.</p>"},{"location":"examples/rust/write_read_dataset/#reading-a-lance-dataset","title":"Reading a Lance dataset","text":"<p>Now that we have written the dataset to a new directory, we can read it back and print out some basic properties.</p> <pre><code>// Reads dataset from the given path and prints batch size, schema for all record batches. Also extracts and prints a slice from the first batch\nasync fn read_dataset(data_path: &amp;str) {\n    let dataset = Dataset::open(data_path).await.unwrap();\n    let scanner = dataset.scan();\n\n    let mut batch_stream = scanner.try_into_stream().await.unwrap().map(|b| b.unwrap());\n\n    while let Some(batch) = batch_stream.next().await {\n        println!(\"Batch size: {}, {}\", batch.num_rows(), batch.num_columns()); // print size of batch\n        println!(\"Schema: {:?}\", batch.schema()); // print schema of recordbatch\n\n        println!(\"Batch: {:?}\", batch); // print the entire recordbatch (schema and data)\n    }\n} // End read dataset\n</code></pre> <p>First we open the dataset, and create a scanner object. We use it to create a <code>batch_stream</code> that will let us access each record batch in the dataset. Then we iterate over the record batches and print out the size and schema of each one.</p>"},{"location":"examples/rust/write_read_dataset/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::UInt32Array;\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::{RecordBatch, RecordBatchIterator};\nuse futures::StreamExt;\nuse lance::dataset::{WriteMode, WriteParams};\nuse lance::Dataset;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    let data_path: &amp;str = \"./temp_data.lance\";\n\n    write_dataset(data_path).await;\n    read_dataset(data_path).await;\n}\n</code></pre>"},{"location":"format/","title":"Lance Format Specification","text":"<p>The Lance format contains both a table format and a columnar file format. When combined, we refer to it as a data format.  Because Lance can store both structured and unstructured multimodal data, Lance typically refers to tables as \"datasets\". A Lance dataset is designed to efficiently handle secondary indices, fast ingestion and modification of data,  and a rich set of schema and data evolution features.</p>"},{"location":"format/#feature-flags","title":"Feature Flags","text":"<p>As the file format and dataset evolve, new feature flags are added to the format.  There are two separate fields for checking for feature flags,  depending on whether you are trying to read or write the table.  Readers should check the <code>reader_feature_flags</code> to see if there are any flag it is not aware of.  Writers should check <code>writer_feature_flags</code>. If either sees a flag they don't know,  they should return an \"unsupported\" error on any read or write operation.</p>"},{"location":"format/file/","title":"File Format","text":""},{"location":"format/file/#file-structure","title":"File Structure","text":"<p>Each <code>.lance</code> file is the container for the actual data.</p> <p></p> <p>At the tail of the file, <code>ColumnMetadata</code> protobuf blocks are used to describe the encoding of the columns in the file.</p> <pre><code>message ColumnMetadata {\n\n  // This describes a page of column data.\n  message Page {\n    // The file offsets for each of the page buffers\n    //\n    // The number of buffers is variable and depends on the encoding.  There\n    // may be zero buffers (e.g. constant encoded data) in which case this\n    // could be empty.\n    repeated uint64 buffer_offsets = 1;\n    // The size (in bytes) of each of the page buffers\n    //\n    // This field will have the same length as `buffer_offsets` and\n    // may be empty.\n    repeated uint64 buffer_sizes = 2;\n    // Logical length (e.g. # rows) of the page\n    uint64 length = 3;\n    // The encoding used to encode the page\n    Encoding encoding = 4;\n    // The priority of the page\n    //\n    // For tabular data this will be the top-level row number of the first row\n    // in the page (and top-level rows should not split across pages).\n    uint64 priority = 5;\n  }\n  // Encoding information about the column itself.  This typically describes\n  // how to interpret the column metadata buffers.  For example, it could\n  // describe how statistics or dictionaries are stored in the column metadata.\n  Encoding encoding = 1;\n  // The pages in the column\n  repeated Page pages = 2;   \n  // The file offsets of each of the column metadata buffers\n  //\n  // There may be zero buffers.\n  repeated uint64 buffer_offsets = 3;\n  // The size (in bytes) of each of the column metadata buffers\n  //\n  // This field will have the same length as `buffer_offsets` and\n  // may be empty.\n  repeated uint64 buffer_sizes = 4;\n\n}\n</code></pre> <p>A <code>Footer</code> describes the overall layout of the file. The entire file layout is described here:</p> <pre><code>// Note: the number of buffers (BN) is independent of the number of columns (CN)\n//       and pages.\n//\n//       Buffers often need to be aligned.  64-byte alignment is common when\n//       working with SIMD operations.  4096-byte alignment is common when\n//       working with direct I/O.  In order to ensure these buffers are aligned\n//       writers may need to insert padding before the buffers.\n//       \n//       If direct I/O is required then most (but not all) fields described\n//       below must be sector aligned.  We have marked these fields with an\n//       asterisk for clarity.  Readers should assume there will be optional\n//       padding inserted before these fields.\n//\n//       All footer fields are unsigned integers written with  little endian\n//       byte order.\n//\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Data Pages                       |\n// |   Data Buffer 0*                 |\n// |   ...                            |\n// |   Data Buffer BN*                |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadatas                 |\n// | |A| Column 0 Metadata*           |\n// |     Column 1 Metadata*           |\n// |     ...                          |\n// |     Column CN Metadata*          |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadata Offset Table     |\n// | |B| Column 0 Metadata Position*  |\n// |     Column 0 Metadata Size       |\n// |     ...                          |\n// |     Column CN Metadata Position  |\n// |     Column CN Metadata Size      |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Global Buffers Offset Table      |\n// | |C| Global Buffer 0 Position*    |\n// |     Global Buffer 0 Size         |\n// |     ...                          |\n// |     Global Buffer GN Position    |\n// |     Global Buffer GN Size        |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Footer                           |\n// | A u64: Offset to column meta 0   |\n// | B u64: Offset to CMO table       |\n// | C u64: Offset to GBO table       |\n// |   u32: Number of global bufs     |\n// |   u32: Number of columns         |\n// |   u16: Major version             |\n// |   u16: Minor version             |\n// |   \"LANC\"                         |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\n// File Layout-End\n</code></pre>"},{"location":"format/file/#file-version","title":"File Version","text":"<p>The Lance file format has gone through a number of changes including a breaking change from version 1 to version 2. There are a number of APIs that allow the file version to be specified. Using a newer version of the file format will lead to better compression and/or performance. However, older software versions may not be able to read newer files.</p> <p>In addition, the latest version of the file format (next) is unstable and should not be used for production use cases. Breaking changes could be made to unstable encodings and that would mean that files written with these encodings are no longer readable by any newer versions of Lance. The <code>next</code> version should only be used for experimentation and benchmarking upcoming features.</p> <p>The following values are supported:</p> Version Minimal Lance Version Maximum Lance Version Description 0.1 Any Any This is the initial Lance format. 2.0 0.16.0 Any Rework of the Lance file format that removed row groups and introduced null support for lists, fixed size lists, and primitives 2.1 (unstable) None Any Enhances integer and string compression, adds support for nulls in struct fields, and improves random access performance with nested fields. legacy N/A N/A Alias for 0.1 stable N/A N/A Alias for the latest stable version (currently 2.0) next N/A N/A Alias for the latest unstable version (currently 2.1)"},{"location":"format/file/#file-encodings","title":"File Encodings","text":"<p>Lance supports a variety of encodings for different data types. The encodings are chosen to give both random access and scan performance. Encodings are added over time and may be extended in the future. The manifest records a max format version which controls which encodings will be used. This allows for a gradual migration to a new data format so that old readers can still read new data while a migration is in progress.</p> <p>Encodings are divided into \"field encodings\" and \"array encodings\". Field encodings are consistent across an entire field of data, while array encodings are used for individual pages of data within a field. Array encodings can nest other array encodings (e.g. a dictionary encoding can bitpack the indices) however array encodings cannot nest field encodings. For this reason data types such as <code>Dictionary&lt;UInt8, List&lt;String&gt;&gt;</code> are not yet supported (since there is no dictionary field encoding)</p>"},{"location":"format/file/#encodings-available","title":"Encodings Available","text":"Encoding Name Encoding Type What it does Supported Versions When it is applied Basic struct Field encoding Encodes non-nullable struct data &gt;= 2.0 Default encoding for structs List Field encoding Encodes lists (nullable or non-nullable) &gt;= 2.0 Default encoding for lists Basic Primitive Field encoding Encodes primitive data types using separate validity array &gt;= 2.0 Default encoding for primitive data types Value Array encoding Encodes a single vector of fixed-width values &gt;= 2.0 Fallback encoding for fixed-width types Binary Array encoding Encodes a single vector of variable-width data &gt;= 2.0 Fallback encoding for variable-width types Dictionary Array encoding Encodes data using a dictionary array and an indices array which is useful for large data types with few unique values &gt;= 2.0 Used on string pages with fewer than 100 unique elements Packed struct Array encoding Encodes a struct with fixed-width fields in a row-major format making random access more efficient &gt;= 2.0 Only used on struct types if the field metadata attribute <code>\"packed\"</code> is set to <code>\"true\"</code> Fsst Array encoding Compresses binary data by identifying common substrings (of 8 bytes or less) and encoding them as symbols &gt;= 2.1 Used on string pages that are not dictionary encoded Bitpacking Array encoding Encodes a single vector of fixed-width values using bitpacking which is useful for integral types that do not span the full range of values &gt;= 2.1 Used on integral types"},{"location":"format/table/","title":"Table Format","text":""},{"location":"format/table/#dataset-directory","title":"Dataset Directory","text":"<p>A <code>Lance Dataset</code> is organized in a directory.</p> <pre><code>/path/to/dataset:\n    data/*.lance  -- Data directory\n    _versions/*.manifest -- Manifest file for each dataset version.\n    _indices/{UUID-*}/index.idx -- Secondary index, each index per directory.\n    _deletions/*.{arrow,bin} -- Deletion files, which contain ids of rows\n      that have been deleted.\n</code></pre> <p>A <code>Manifest</code> file includes the metadata to describe a version of the dataset.</p> <pre><code>message Manifest {\n  // All fields of the dataset, including the nested fields.\n  repeated lance.file.Field fields = 1;\n\n  // Fragments of the dataset.\n  repeated DataFragment fragments = 2;\n\n  // Snapshot version number.\n  uint64 version = 3;\n\n  // The file position of the version auxiliary data.\n  //  * It is not inheritable between versions.\n  //  * It is not loaded by default during query.\n  uint64 version_aux_data = 4;\n\n  // Schema metadata.\n  map&lt;string, bytes&gt; metadata = 5;\n\n  message WriterVersion {\n    // The name of the library that created this file.\n    string library = 1;\n    // The version of the library that created this file. Because we cannot assume\n    // that the library is semantically versioned, this is a string. However, if it\n    // is semantically versioned, it should be a valid semver string without any 'v'\n    // prefix. For example: `2.0.0`, `2.0.0-rc.1`.\n    string version = 2;\n  }\n\n  // The version of the writer that created this file.\n  //\n  // This information may be used to detect whether the file may have known bugs\n  // associated with that writer.\n  WriterVersion writer_version = 13;\n\n  // If present, the file position of the index metadata.\n  optional uint64 index_section = 6;\n\n  // Version creation Timestamp, UTC timezone\n  google.protobuf.Timestamp timestamp = 7;\n\n  // Optional version tag\n  string tag = 8;\n\n  // Feature flags for readers.\n  //\n  // A bitmap of flags that indicate which features are required to be able to\n  // read the table. If a reader does not recognize a flag that is set, it\n  // should not attempt to read the dataset.\n  //\n  // Known flags:\n  // * 1: deletion files are present\n  // * 2: move_stable_row_ids: row IDs are tracked and stable after move operations\n  //       (such as compaction), but not updates.\n  // * 4: use v2 format (deprecated)\n  // * 8: table config is present\n  uint64 reader_feature_flags = 9;\n\n  // Feature flags for writers.\n  //\n  // A bitmap of flags that indicate which features must be used when writing to the\n  // dataset. If a writer does not recognize a flag that is set, it should not attempt to\n  // write to the dataset.\n  //\n  // The flag identities are the same as for reader_feature_flags, but the values of\n  // reader_feature_flags and writer_feature_flags are not required to be identical.\n  uint64 writer_feature_flags = 10;\n\n  // The highest fragment ID that has been used so far.\n  //\n  // This ID is not guaranteed to be present in the current version, but it may\n  // have been used in previous versions.\n  //\n  // For a single fragment, will be zero. For no fragments, will be absent.\n  optional uint32 max_fragment_id = 11;\n\n  // Path to the transaction file, relative to `{root}/_transactions`. The file at that\n  // location contains a wire-format serialized Transaction message representing the\n  // transaction that created this version.\n  //\n  // This string field \"transaction_file\" may be empty if no transaction file was written.\n  //\n  // The path format is \"{read_version}-{uuid}.txn\" where {read_version} is the version of\n  // the table the transaction read from (serialized to decimal with no padding digits),\n  // and {uuid} is a hyphen-separated UUID.\n  string transaction_file = 12;\n\n  // The next unused row id. If zero, then the table does not have any rows.\n  //\n  // This is only used if the \"move_stable_row_ids\" feature flag is set.\n  uint64 next_row_id = 14;\n\n  message DataStorageFormat {\n    // The format of the data files (e.g. \"lance\")\n    string file_format = 1;\n    // The max format version of the data files. The format of the version can vary by\n    // file_format and is not required to follow semver.\n    //\n    // Every file in this version of the dataset has the same file_format version.\n    string version = 2;\n  }\n\n  // The data storage format\n  //\n  // This specifies what format is used to store the data files.\n  DataStorageFormat data_format = 15;\n\n  // Table config.\n  //\n  // Keys with the prefix \"lance.\" are reserved for the Lance library. Other\n  // libraries may wish to similarly prefix their configuration keys\n  // appropriately.\n  map&lt;string, string&gt; config = 16;\n\n  // The version of the blob dataset associated with this table.  Changes to\n  // blob fields will modify the blob dataset and update this version in the parent\n  // table.\n  //\n  // If this value is 0 then there are no blob fields.\n  uint64 blob_dataset_version = 17;\n\n\n}\n</code></pre>"},{"location":"format/table/#fragments","title":"Fragments","text":"<p><code>DataFragment</code> represents a chunk of data in the dataset. Itself includes one or more <code>DataFile</code>, where each <code>DataFile</code> can contain several columns in the chunk of data. It also may include a <code>DeletionFile</code>, which is explained in a later section.</p> <pre><code>message DataFragment {\n  // The ID of a DataFragment is unique within a dataset.\n  uint64 id = 1;\n\n  repeated DataFile files = 2;\n\n  // File that indicates which rows, if any, should be considered deleted.\n  DeletionFile deletion_file = 3;\n\n  // TODO: What's the simplest way we can allow an inline tombstone bitmap?\n\n  // A serialized RowIdSequence message (see rowids.proto).\n  //\n  // These are the row ids for the fragment, in order of the rows as they appear.\n  // That is, if a fragment has 3 rows, and the row ids are [1, 42, 3], then the\n  // first row is row 1, the second row is row 42, and the third row is row 3.\n  oneof row_id_sequence {\n    // If small (&lt; 200KB), the row ids are stored inline.\n    bytes inline_row_ids = 5;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_row_ids = 6;\n  } // row_id_sequence\n\n  // Number of original rows in the fragment, this includes rows that are now marked with\n  // deletion tombstones. To compute the current number of rows, subtract\n  // `deletion_file.num_deleted_rows` from this value.\n  uint64 physical_rows = 4;\n\n}\n</code></pre> <p>The overall structure of a fragment is shown below. One or more data files store the columns of a fragment. New columns can be added to a fragment by adding new data files. The deletion file (if present), stores the rows that have been deleted from the fragment.</p> <p></p> <p>Every row has a unique id, which is an u64 that is composed of two u32s: the fragment id and the local row id. The local row id is just the index of the row in the data files.</p>"},{"location":"format/table/#dataset-update-and-data-evolution","title":"Dataset Update and Data Evolution","text":"<p><code>Lance</code> supports fast dataset update and schema evolution via manipulating the <code>Manifest</code> metadata.</p> <p><code>Appending</code> is done by appending new <code>Fragment</code> to the dataset. While adding columns is done by adding new <code>DataFile</code> of the new columns to each <code>Fragment</code>. Finally, <code>Overwrite</code> a dataset can be done by resetting the <code>Fragment</code> list of the <code>Manifest</code>.</p> <p></p>"},{"location":"format/table/#schema-fields","title":"Schema &amp; Fields","text":"<p>Fields represent the metadata for a column. This includes the name, data type, id, nullability, and encoding.</p> <p>Fields are listed in depth first order, and can be one of:</p> <ol> <li>parent (struct)</li> <li>repeated (list/array)</li> <li>leaf (primitive)</li> </ol> <p>For example, the schema:</p> <pre><code>a: i32\nb: struct {\n    c: list&lt;i32&gt;\n    d: i32\n}\n</code></pre> <p>Would be represented as the following field list:</p> name id type parent_id logical_type <code>a</code> 1 LEAF 0 <code>\"int32\"</code> <code>b</code> 2 PARENT 0 <code>\"struct\"</code> <code>b.c</code> 3 REPEATED 2 <code>\"list\"</code> <code>b.c</code> 4 LEAF 3 <code>\"int32\"</code> <code>b.d</code> 5 LEAF 2 <code>\"int32\"</code>"},{"location":"format/table/#field-encoding-specification","title":"Field Encoding Specification","text":"<p>Column-level encoding configurations are specified through PyArrow field metadata:</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema([\n    pa.field(\n        \"compressible_strings\",\n        pa.string(),\n        metadata={\n            \"lance-encoding:compression\": \"zstd\",\n            \"lance-encoding:compression-level\": \"3\",\n            \"lance-encoding:structural-encoding\": \"miniblock\",\n            \"lance-encoding:packed\": \"true\"\n        }\n    )\n])\n</code></pre> Metadata Key Type Description Example Values Example Usage (Python) <code>lance-encoding:compression</code> Compression Specifies compression algorithm zstd <code>metadata={\"lance-encoding:compression\": \"zstd\"}</code> <code>lance-encoding:compression-level</code> Compression Zstd compression level (1-22) 3 <code>metadata={\"lance-encoding:compression-level\": \"3\"}</code> <code>lance-encoding:blob</code> Storage Marks binary data (&gt;4MB) for chunked storage true/false <code>metadata={\"lance-encoding:blob\": \"true\"}</code> <code>lance-encoding:packed</code> Optimization Struct memory layout optimization true/false <code>metadata={\"lance-encoding:packed\": \"true\"}</code> <code>lance-encoding:structural-encoding</code> Nested Data Encoding strategy for nested structures miniblock/fullzip <code>metadata={\"lance-encoding:structural-encoding\": \"miniblock\"}</code>"},{"location":"format/table/#deletion","title":"Deletion","text":"<p>Rows can be marked deleted by adding a deletion file next to the data in the <code>_deletions</code> folder. These files contain the indices of rows that have between deleted for some fragment. For a given version of the dataset, each fragment can have up to one deletion file. Fragments that have no deleted rows have no deletion file.</p> <p>Readers should filter out row ids contained in these deletion files during a scan or ANN search.</p> <p>Deletion files come in two flavors:</p> <ol> <li>Arrow files: which store a column with a flat vector of indices</li> <li>Roaring bitmaps: which store the indices as compressed bitmaps.</li> </ol> <p>Roaring Bitmaps are used for larger deletion sets, while Arrow files are used for small ones. This is because Roaring Bitmaps are known to be inefficient for small sets.</p> <p>The filenames of deletion files are structured like:</p> <pre><code>_deletions/{fragment_id}-{read_version}-{random_id}.{arrow|bin}\n</code></pre> <p>Where <code>fragment_id</code> is the fragment the file corresponds to, <code>read_version</code> is the version of the dataset that it was created off of (usually one less than the version it was committed to), and <code>random_id</code> is a random i64 used to avoid collisions. The suffix is determined by the file type (<code>.arrow</code> for Arrow file, <code>.bin</code> for roaring bitmap).</p> <pre><code>message DeletionFile {\n  // Type of deletion file, which varies depending on what is the most efficient\n  // way to store the deleted row offsets. If none, then will be unspecified. If there are\n  // sparsely deleted rows, then ARROW_ARRAY is the most efficient. If there are\n  // densely deleted rows, then BIT_MAP is the most efficient.\n  enum DeletionFileType {\n    // Deletion file is a single Int32Array of deleted row offsets. This is stored as\n    // an Arrow IPC file with one batch and one column. Has a .arrow extension.\n    ARROW_ARRAY = 0;\n    // Deletion file is a Roaring Bitmap of deleted row offsets. Has a .bin extension.\n    BITMAP = 1;\n  }\n\n  // Type of deletion file. If it is unspecified, then the remaining fields will be missing.\n  DeletionFileType file_type = 1;\n  // The version of the dataset this deletion file was built from.\n  uint64 read_version = 2;\n  // An opaque id used to differentiate this file from others written by concurrent\n  // writers.\n  uint64 id = 3;\n  // The number of rows that are marked as deleted.\n  uint64 num_deleted_rows = 4;\n\n}\n</code></pre> <p>Deletes can be materialized by re-writing data files with the deleted rows removed. However, this invalidates row indices and thus the ANN indices, which can be expensive to recompute.</p>"},{"location":"format/table/#committing-datasets","title":"Committing Datasets","text":"<p>A new version of a dataset is committed by writing a new manifest file to the <code>_versions</code> directory.</p> <p>To prevent concurrent writers from overwriting each other, the commit process must be atomic and consistent for all writers. If two writers try to commit using different mechanisms, they may overwrite each other's changes. For any storage system that natively supports atomic rename-if-not-exists or put-if-not-exists, these operations should be used. This is true of local file systems and most cloud object stores including Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage. For ones that lack this functionality, an external locking mechanism can be configured by the user.</p>"},{"location":"format/table/#manifest-naming-schemes","title":"Manifest Naming Schemes","text":"<p>Manifest files must use a consistent naming scheme. The names correspond to the versions. That way we can open the right version of the dataset without having to read all the manifests. It also makes it clear which file path is the next one to be written.</p> <p>There are two naming schemes that can be used:</p> <ol> <li>V1: <code>_versions/{version}.manifest</code>. This is the legacy naming scheme.</li> <li>V2: <code>_versions/{u64::MAX - version:020}.manifest</code>. This is the new naming scheme.    The version is zero-padded (to 20 digits) and subtracted from <code>u64::MAX</code>.    This allows the versions to be sorted in descending order,    making it possible to find the latest manifest on object storage using a single list call.</li> </ol> <p>It is an error for there to be a mixture of these two naming schemes.</p>"},{"location":"format/table/#conflict-resolution","title":"Conflict Resolution","text":"<p>If two writers try to commit at the same time, one will succeed and the other will fail. The failed writer should attempt to retry the commit, but only if its changes are compatible with the changes made by the successful writer.</p> <p>The changes for a given commit are recorded as a transaction file, under the <code>_transactions</code> prefix in the dataset directory. The transaction file is a serialized <code>Transaction</code> protobuf message. See the <code>transaction.proto</code> file for its definition.</p> <p></p> <p>The commit process is as follows:</p> <ol> <li>The writer finishes writing all data files.</li> <li>The writer creates a transaction file in the <code>_transactions</code> directory.    This file describes the operations that were performed, which is used for two purposes:    (1) to detect conflicts, and (2) to re-build the manifest during retries.</li> <li>Look for any new commits since the writer started writing.    If there are any, read their transaction files and check for conflicts.    If there are any conflicts, abort the commit. Otherwise, continue.</li> <li>Build a manifest and attempt to commit it to the next version.    If the commit fails because another writer has already committed, go back to step 3.</li> </ol> <p>When checking whether two transactions conflict, be conservative. If the transaction file is missing, assume it conflicts. If the transaction file has an unknown operation, assume it conflicts.</p>"},{"location":"format/table/#external-manifest-store","title":"External Manifest Store","text":"<p>If the backing object store does not support *-if-not-exists operations, an external manifest store can be used to allow concurrent writers. An external manifest store is a KV store that supports put-if-not-exists operation. The external manifest store supplements but does not replace the manifests in object storage. A reader unaware of the external manifest store could read a table that uses it, but it might be up to one version behind the true latest version of the table.</p> <p></p> <p>The commit process is as follows:</p> <ol> <li><code>PUT_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid}</code> stage a new manifest in object store under a unique path determined by new uuid</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest-{uuid}</code> commit the path of the staged manifest to the external store.</li> <li><code>COPY_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid} mydataset.lance/_versions/{version}.manifest</code> copy the staged manifest to the final path</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest</code> update the external store to point to the final manifest</li> </ol> <p>Note that the commit is effectively complete after step 2. If the writer fails after step 2, a reader will be able to detect the external store and object store are out-of-sync, and will try to synchronize the two stores. If the reattempt at synchronization fails, the reader will refuse to load. This is to ensure that the dataset is always portable by copying the dataset directory without special tool.</p> <p></p> <p>The reader load process is as follows:</p> <ol> <li><code>GET_EXTERNAL_STORE base_uri, version, path</code> then, if path does not end in a UUID return the path</li> <li><code>COPY_OBJECT_STORE mydataset.lance/_versions/{version}.manifest-{uuid} mydataset.lance/_versions/{version}.manifest</code> reattempt synchronization</li> <li><code>PUT_EXTERNAL_STORE base_uri, version, mydataset.lance/_versions/{version}.manifest</code> update the external store to point to the final manifest</li> <li><code>RETURN mydataset.lance/_versions/{version}.manifest</code> always return the finalized path, return error if synchronization fails</li> </ol>"},{"location":"format/table/#feature-move-stable-row-ids","title":"Feature: Move-Stable Row IDs","text":"<p>The row ids features assigns a unique u64 id to each row in the table.  This id is stable after being moved (such as during compaction),  but is not necessarily stable after a row is updated. (A future feature may make them stable after updates.)  To make access fast, a secondary index is created that maps row ids to their locations in the table.  The respective parts of these indices are stored in the respective fragment's metadata.</p> <p>row id : A unique auto-incrementing u64 id assigned to each row in the table.</p> <p>row address : The current location of a row in the table. This is a u64 that can be thought of as a pair of two u32 values: the fragment id and the local row offset. For example, if the row address is (42, 9), then the row is in the 42rd fragment and is the 10th row in that fragment.</p> <p>row id sequence : The sequence of row ids in a fragment.</p> <p>row id index : A secondary index that maps row ids to row addresses. This index is constructed by reading all the row id sequences.</p>"},{"location":"format/table/#assigning-row-ids","title":"Assigning Row IDs","text":"<p>Row ids are assigned in a monotonically increasing sequence. The next row id is stored in the manifest as the field <code>next_row_id</code>. This starts at zero. When making a commit, the writer uses that field to assign row ids to new fragments. If the commit fails, the writer will re-read the new <code>next_row_id</code>, update the new row ids, and then try again. This is similar to how the <code>max_fragment_id</code> is used to assign new fragment ids.</p> <p>When a row id updated, it is typically assigned a new row id rather than reusing the old one. This is because this feature doesn't have a mechanism to update secondary indices that may reference the old values for the row id. By deleting the old row id and creating a new one, the secondary indices will avoid referencing stale data.</p>"},{"location":"format/table/#row-id-sequences","title":"Row ID Sequences","text":"<p>The row id values for a fragment are stored in a <code>RowIdSequence</code> protobuf message. This is described in the protos/rowids.proto file. Row id sequences are just arrays of u64 values, which have representations optimized for the common case where they are sorted and possibly contiguous. For example, a new fragment will have a row id sequence that is just a simple range, so it is stored as a <code>start</code> and <code>end</code> value.</p> <p>These sequence messages are either stored inline in the fragment metadata, or are written to a separate file and referenced from the fragment metadata. This choice is typically made based on the size of the sequence. If the sequence is small, it is stored inline. If it is large, it is written to a separate file. By keeping the small sequences inline, we can avoid the overhead of additional IO operations.</p> <pre><code>oneof row_id_sequence {\n    // Inline sequence\n    bytes inline_sequence = 1;\n    // External file reference\n    string external_file = 2;\n} // row_id_sequence\n</code></pre>"},{"location":"format/table/#row-id-index","title":"Row ID Index","text":"<p>To ensure fast access to rows by their row id, a secondary index is created that maps row ids to their locations in the table. This index is built when a table is loaded, based on the row id sequences in the fragments. For example, if fragment 42 has a row id sequence of <code>[0, 63, 10]</code>, then the index will have entries for <code>0 -&gt; (42, 0)</code>, <code>63 -&gt; (42, 1)</code>, <code>10 -&gt; (42, 2)</code>. The exact form of this index is left up to the implementation, but it should be optimized for fast lookups. </p>"},{"location":"guide/arrays/","title":"Extension Arrays","text":"<p>Lance provides extensions for Arrow arrays and Pandas Series to represent data types for machine learning applications.</p>"},{"location":"guide/arrays/#bfloat16","title":"BFloat16","text":"<p>BFloat16 is a 16-bit floating point number that is designed for machine learning use cases. Intuitively, it only has 2-3 digits of precision, but it has the same range as a 32-bit float: ~1e-38 to ~1e38. By comparison, a 16-bit float has a range of ~5.96e-8 to 65504.</p> <p>Lance provides an Arrow extension array (<code>lance.arrow.BFloat16Array</code>) and a Pandas extension array (<code>lance._arrow.PandasBFloat16Type</code>) for BFloat16. These are compatible with the ml_dtypes bfloat16 NumPy extension array.</p> <p>If you are using Pandas, you can use the <code>lance.bfloat16</code> dtype string to create the array:</p> <pre><code>import lance.arrow\n\npd.Series([1.1, 2.1, 3.4], dtype=\"lance.bfloat16\")\n# 0    1.1015625\n# 1      2.09375\n# 2      3.40625\n# dtype: lance.bfloat16\n</code></pre> <p>To create an Arrow array, use the <code>lance.arrow.bfloat16_array</code> function:</p> <pre><code>from lance.arrow import bfloat16_array\n\nbfloat16_array([1.1, 2.1, 3.4])\n# &lt;lance.arrow.BFloat16Array object at 0x000000016feb94e0&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>Finally, if you have a pre-existing NumPy array, you can convert it into either:</p> <pre><code>import numpy as np\nfrom ml_dtypes import bfloat16\nfrom lance.arrow import PandasBFloat16Array, BFloat16Array\n\nnp_array = np.array([1.1, 2.1, 3.4], dtype=bfloat16)\nPandasBFloat16Array.from_numpy(np_array)\n# &lt;PandasBFloat16Array&gt;\n# [1.1015625, 2.09375, 3.40625]\n# Length: 3, dtype: lance.bfloat16\nBFloat16Array.from_numpy(np_array)\n# &lt;lance.arrow.BFloat16Array object at 0x...&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>When reading, these can be converted back to to the NumPy bfloat16 dtype using each array class's <code>to_numpy</code> method.</p>"},{"location":"guide/arrays/#imageuri","title":"ImageURI","text":"<p><code>lance.arrow.ImageURIArray</code> is an array that stores the URI location of images in some other storage system. For example, <code>file:///path/to/image.png</code> for a local filesystem or <code>s3://bucket/path/image.jpeg</code> for an image on AWS S3. Use this array type when you want to lazily load images from an existing storage medium.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.from_uris</code> with a list of URIs represented by either <code>pyarrow.StringArray</code> or an iterable that yields strings. Note that the URIs are not strongly validated and images are not read into memory automatically.</p> <pre><code>from lance.arrow import ImageURIArray\n\nImageURIArray.from_uris([\n   \"/tmp/image1.jpg\",\n   \"file:///tmp/image2.jpg\",\n   \"s3://example/image3.jpg\"\n])\n# &lt;lance.arrow.ImageURIArray object at 0x...&gt;\n# ['/tmp/image1.jpg', 'file:///tmp/image2.jpg', 's3://example/image3.jpg']\n</code></pre> <p><code>lance.arrow.ImageURIArray.read_uris</code> will read images into memory and return them as a new <code>lance.arrow.EncodedImageArray</code> object.</p> <pre><code>from lance.arrow import ImageURIArray\n\nrelative_path = \"images/1.png\"\nuris = [os.path.join(os.path.dirname(__file__), relative_path)]\nImageURIArray.from_uris(uris).read_uris()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00...']\n</code></pre>"},{"location":"guide/arrays/#encodedimage","title":"EncodedImage","text":"<p><code>lance.arrow.EncodedImageArray</code> is an array that stores jpeg and png images in their encoded and compressed representation as they would appear written on disk. Use this array when you want to manipulate images in their compressed format such as when you're reading them from disk or embedding them into HTML.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.read_uris</code> on an existing <code>lance.arrow.ImageURIArray</code>. This will read the referenced images into memory. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing it an array of encoded images already read into <code>pyarrow.BinaryArray</code> or by calling <code>lance.arrow.ImageTensorArray.to_encoded</code>.</p> <p>A <code>lance.arrow.EncodedImageArray.to_tensor</code> method is provided to decode encoded images and return them as <code>lance.arrow.FixedShapeImageTensorArray</code>, from which they can be converted to numpy arrays or TensorFlow tensors. For decoding images, it will first attempt to use a decoder provided via the optional function parameter. If decoder is not provided it will attempt to use Pillow and tensorflow in that order. If neither library or custom decoder is available an exception will be raised.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [os.path.join(os.path.dirname(__file__), \"images/1.png\")]\nencoded_images = ImageURIArray.from_uris(uris).read_uris()\nprint(encoded_images.to_tensor())\n\ndef tensorflow_decoder(images):\n    import tensorflow as tf\n    import numpy as np\n\n    return np.stack(tf.io.decode_png(img.as_py(), channels=3) for img in images.storage)\n\nprint(encoded_images.to_tensor(tensorflow_decoder))\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n</code></pre>"},{"location":"guide/arrays/#fixedshapeimagetensor","title":"FixedShapeImageTensor","text":"<p><code>lance.arrow.FixedShapeImageTensorArray</code> is an array that stores images as tensors where each individual pixel is represented as a numeric value. Typically images are stored as 3 dimensional tensors shaped (height, width, channels). In color images each pixel is represented by three values (channels) as per RGB color model. Images from this array can be read out as numpy arrays individually or stacked together into a single 4 dimensional numpy array shaped (batch_size, height, width, channels).</p> <p>It can be created by calling <code>lance.arrow.EncodedImageArray.to_tensor</code> on a previously existing <code>lance.arrow.EncodedImageArray</code>. This will decode encoded images and return them as a <code>lance.arrow.FixedShapeImageTensorArray</code>. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing in a <code>pyarrow.FixedShapeTensorArray</code>.</p> <p>It can be encoded into to <code>lance.arrow.EncodedImageArray</code> by calling <code>lance.arrow.FixedShapeImageTensorArray.to_encoded</code> and passing custom encoder If encoder is not provided it will attempt to use tensorflow and Pillow in that order. Default encoders will encode to PNG. If neither library is available it will raise an exception.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [image_uri]\ntensor_images = ImageURIArray.from_uris(uris).read_uris().to_tensor()\ntensor_images.to_encoded()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [...\n# b'\\x89PNG\\r\\n\\x1a...'\n</code></pre>"},{"location":"guide/blob/","title":"Blob As Files","text":"<p>Unlike other data formats, large multimodal data is a first-class citizen in the Lance columnar format. Lance provides a high-level API to store and retrieve large binary objects (blobs) in Lance datasets.</p> <p></p> <p>Lance serves large binary data using <code>lance.BlobFile</code>, which is a file-like object that lazily reads large binary objects.</p> <p>To create a Lance dataset with large blob data, you can mark a large binary column as a blob column by adding the metadata <code>lance-encoding:blob</code> to <code>true</code>.</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"video\",\n            pa.large_binary(),\n            metadata={\"lance-encoding:blob\": \"true\"}\n        ),\n    ]\n)\n</code></pre> <p>To write blob data to a Lance dataset, create a PyArrow table with the blob schema and use <code>lance.write_dataset</code>:</p> <pre><code>import lance\n\n# First, download a sample video file for testing\n# wget https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\nimport urllib.request\nurllib.request.urlretrieve(\n    \"https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\",\n    \"sample_video.mp4\"\n)\n\n# Then read the video file content\nwith open(\"sample_video.mp4\", 'rb') as f:\n    video_data = f.read()\n\n# Create table with blob data\ntable = pa.table({\n    \"id\": [1],\n    \"video\": [video_data],\n}, schema=schema)\n\n# Write to Lance dataset\nds = lance.write_dataset(\n    table,\n    \"./youtube.lance\",\n    schema=schema\n)\n</code></pre> <p>To fetch blobs from a Lance dataset, you can use <code>lance.dataset.LanceDataset.take_blobs</code>.</p> <p>For example, it's easy to use <code>BlobFile</code> to extract frames from a video file without loading the entire video into memory.</p> <pre><code>import av # pip install av\nimport lance\n\nds = lance.dataset(\"./youtube.lance\")\nstart_time, end_time = 500, 1000\n# Get blob data from the first row (id=0)\nblobs = ds.take_blobs(\"video\", ids=[0])\nwith av.open(blobs[0]) as container:\n    stream = container.streams.video[0]\n    stream.codec_context.skip_frame = \"NONKEY\"\n\n    start_time = start_time / stream.time_base\n    start_time = start_time.as_integer_ratio()[0]\n    end_time = end_time / stream.time_base\n    container.seek(start_time, stream=stream)\n\n    for frame in container.decode(stream):\n        if frame.time &gt; end_time:\n            break\n        display(frame.to_image())\n        clear_output(wait=True) \n</code></pre>"},{"location":"guide/data_evolution/","title":"Data Evolution","text":"<p>Lance supports traditional schema evolution: adding, removing, and altering columns in a dataset. Most of these operations can be performed without rewriting the data files in the dataset, making them very efficient operations. In addition, Lance supports data evolution, which allows you to also backfill existing rows with the new column data without rewriting the data files in the dataset, making it highly suitable for use cases like ML feature engineering.</p> <p>In general, schema changes will conflict with most other concurrent write operations. For example, if you change the schema of the dataset while someone else is appending data to it, either your schema change or the append will fail, depending on the order of the operations. Thus, it's recommended to perform schema changes when no other writes are happening.</p>"},{"location":"guide/data_evolution/#adding-new-columns","title":"Adding new columns","text":""},{"location":"guide/data_evolution/#schema-only","title":"Schema only","text":"<p>A common use case we've seen in production is to add a new column to a dataset without populating it. This is useful to later run a large distributed job to populate the column lazily. To do this, you can use the <code>lance.LanceDataset.add_columns</code> method to add columns with <code>pyarrow.Field</code> or <code>pyarrow.Schema</code>.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"null_columns\")\n\n# With pyarrow Field\ndataset.add_columns(pa.field(\"embedding\", pa.list_(pa.float32(), 128)))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n])\n\n# With pyarrow Schema\ndataset.add_columns(pa.schema([\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n]))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n])\n</code></pre> <p>This operation is very fast, as it only updates the metadata of the dataset.</p>"},{"location":"guide/data_evolution/#with-data-backfill","title":"With data backfill","text":"<p>New columns can be added and populated within a single operation using the <code>lance.LanceDataset.add_columns</code> method. There are two ways to specify how to populate the new columns: first, by providing a SQL expression for each new column, or second, by providing a function to generate the new column data.</p> <p>SQL expressions can either be independent expressions or reference existing columns. SQL literal values can be used to set a single value for all existing rows.</p> <pre><code>table = pa.table({\"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\")\ndataset.add_columns({\n    \"hash\": \"sha256(name)\",\n    \"status\": \"'active'\",\n})\nprint(dataset.to_table().to_pandas())\n#     name                                               hash  status\n# 0  Alice  b';\\xc5\\x10b\\x97&lt;E\\x8dZo-\\x8dd\\xa0#$cT\\xad~\\x0...  active\n# 1    Bob  b'\\xcd\\x9f\\xb1\\xe1H\\xcc\\xd8D.Z\\xa7I\\x04\\xccs\\x...  active\n# 2  Carla  b'\\xad\\x8d\\x83\\xff\\xd8+Z\\x8e\\xd4)\\xe8Y+\\\\\\xb3\\...  active\n</code></pre> <p>You can also provide a Python function to generate the new column data. This can be used, for example, to compute a new embedding column. This function should take a PyArrow RecordBatch and return either a PyArrow RecordBatch or a Pandas DataFrame. The function will be called once for each batch in the dataset.</p> <p>If the function is expensive to compute and can fail, it is recommended to set a checkpoint file in the UDF. This checkpoint file saves the state of the UDF after each invocation, so that if the UDF fails, it can be restarted from the last checkpoint. Note that this file can get quite large, since it needs to store unsaved results for up to an entire data file.</p> <pre><code>import lance\nimport pyarrow as pa\nimport numpy as np\n\ntable = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\n\n@lance.batch_udf(checkpoint_file=\"embedding_checkpoint.sqlite\")\ndef add_random_vector(batch):\n    embeddings = np.random.rand(batch.num_rows, 128).astype(\"float32\")\n    return pd.DataFrame({\"embedding\": embeddings})\ndataset.add_columns(add_random_vector)\n</code></pre>"},{"location":"guide/data_evolution/#using-merge","title":"Using merge","text":"<p>If you have pre-computed one or more new columns, you can add them to an existing dataset using the <code>lance.LanceDataset.merge</code> method. This allows filling in additional columns without having to rewrite the whole dataset.</p> <p>To use the <code>merge</code> method, provide a new dataset that includes the columns you want to add, and a column name to use for joining the new data to the existing dataset.</p> <p>For example, imagine we have a dataset of embeddings and ids:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.array([np.array([1, 2, 3]), np.array([4, 5, 6]),\n                          np.array([7, 8, 9])])\n})\ndataset = lance.write_dataset(table, \"embeddings\", mode=\"overwrite\")\n</code></pre> <p>Now if we want to add a column of labels we have generated, we can do so by merging a new table:</p> <pre><code>new_data = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"label\": pa.array([\"horse\", \"rabbit\", \"cat\"])\n})\ndataset.merge(new_data, \"id\")\nprint(dataset.to_table().to_pandas())\n#    id  embedding   label\n# 0   1  [1, 2, 3]   horse\n# 1   2  [4, 5, 6]  rabbit\n# 2   3  [7, 8, 9]     cat\n</code></pre>"},{"location":"guide/data_evolution/#dropping-columns","title":"Dropping columns","text":"<p>Finally, you can drop columns from a dataset using the <code>lance.LanceDataset.drop_columns</code> method. This is a metadata-only operation and does not delete the data on disk. This makes it very quick.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3]),\n                 \"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\", mode=\"overwrite\")\ndataset.drop_columns([\"name\"])\nprint(dataset.schema)\n# id: int64\n</code></pre> <p>To actually remove the data from disk, the files must be rewritten to remove the columns and then the old files must be deleted. This can be done using <code>lance.dataset.DatasetOptimizer.compact_files()</code> followed by <code>lance.LanceDataset.cleanup_old_versions()</code>.</p>"},{"location":"guide/data_evolution/#renaming-columns","title":"Renaming columns","text":"<p>Columns can be renamed using the <code>lance.LanceDataset.alter_columns</code> method.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\ndataset.alter_columns({\"path\": \"id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#    new_id\n# 0       1\n# 1       2\n# 2       3\n</code></pre> <p>This works for nested columns as well. To address a nested column, use a dot (<code>.</code>) to separate the levels of nesting. For example:</p> <pre><code>data = [\n  {\"meta\": {\"id\": 1, \"name\": \"Alice\"}},\n  {\"meta\": {\"id\": 2, \"name\": \"Bob\"}},\n]\nschema = pa.schema([\n    (\"meta\", pa.struct([\n        (\"id\", pa.int32()),\n        (\"name\", pa.string()),\n    ]))\n])\ndataset = lance.write_dataset(data, \"nested_rename\")\ndataset.alter_columns({\"path\": \"meta.id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#                                  meta\n# 0  {'new_id': 1, 'name': 'Alice'}\n# 1    {'new_id': 2, 'name': 'Bob'}\n</code></pre>"},{"location":"guide/data_evolution/#casting-column-data-types","title":"Casting column data types","text":"<p>In addition to changing column names, you can also change the data type of a column using the <code>lance.LanceDataset.alter_columns</code> method. This requires rewriting that column to new data files, but does not require rewriting the other columns.</p> <p>Note</p> <p>If the column has an index, the index will be dropped if the column type is changed.</p> <p>This method can be used to change the vector type of a column. For example, we can change a float32 embedding column into a float16 column to save disk space at the cost of lower precision:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.FixedShapeTensorArray.from_numpy_ndarray(\n       np.random.rand(3, 128).astype(\"float32\"))\n})\ndataset = lance.write_dataset(table, \"embeddings\")\ndataset.alter_columns({\"path\": \"embedding\",\n                       \"data_type\": pa.list_(pa.float16(), 128)})\nprint(dataset.schema)\n# id: int64\n# embedding: fixed_size_list&lt;item: halffloat&gt;[128]\n#   child 0, item: halffloat\n</code></pre>"},{"location":"guide/distributed_write/","title":"Distributed Write","text":"<p>Warning</p> <p>Lance provides out-of-the-box Ray and Spark integrations.</p> <p>This page is intended for users who wish to perform distributed operations in a custom manner, i.e. using <code>slurm</code> or <code>Kubernetes</code> without the Lance integration.</p>"},{"location":"guide/distributed_write/#overview","title":"Overview","text":"<p>The Lance format is designed to support parallel writing across multiple distributed workers. A distributed write operation can be performed by two phases:</p> <ol> <li>Parallel Writes: Generate new <code>lance.LanceFragment</code> in parallel across multiple workers.</li> <li>Commit: Collect all the <code>lance.FragmentMetadata</code> and commit into a single dataset in a single <code>lance.LanceOperation</code>.</li> </ol> <p></p>"},{"location":"guide/distributed_write/#write-new-data","title":"Write new data","text":"<p>Writing or appending new data is straightforward with <code>lance.fragment.write_fragments</code>.</p> <pre><code>import json\nfrom lance.fragment import write_fragments\n\n# Run on each worker\ndata_uri = \"./dist_write\"\nschema = pa.schema([\n    (\"a\", pa.int32()),\n    (\"b\", pa.string()),\n])\n\n# Run on worker 1\ndata1 = {\n    \"a\": [1, 2, 3],\n    \"b\": [\"x\", \"y\", \"z\"],\n}\nfragments_1 = write_fragments(data1, data_uri, schema=schema)\nprint(\"Worker 1: \", fragments_1)\n\n# Run on worker 2\ndata2 = {\n    \"a\": [4, 5, 6],\n    \"b\": [\"u\", \"v\", \"w\"],\n}\nfragments_2 = write_fragments(data2, data_uri, schema=schema)\nprint(\"Worker 2: \", fragments_2)\n</code></pre> <p>Output: <pre><code>Worker 1:  [FragmentMetadata(id=0, files=...)]\nWorker 2:  [FragmentMetadata(id=0, files=...)]\n</code></pre></p> <p>Now, use <code>lance.fragment.FragmentMetadata.to_json</code> to serialize the fragment metadata, and collect all serialized metadata on a single worker to execute the final commit operation.</p> <pre><code>import json\nfrom lance import FragmentMetadata, LanceOperation\n\n# Serialize Fragments into JSON data\nfragments_json1 = [json.dumps(fragment.to_json()) for fragment in fragments_1]\nfragments_json2 = [json.dumps(fragment.to_json()) for fragment in fragments_2]\n\n# On one worker, collect all fragments\nall_fragments = [FragmentMetadata.from_json(f) for f in \\\n    fragments_json1 + fragments_json2]\n\n# Commit the fragments into a single dataset\n# Use LanceOperation.Overwrite to overwrite the dataset or create new dataset.\nop = lance.LanceOperation.Overwrite(schema, all_fragments)\nread_version = 0 # Because it is empty at the time.\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n\n# We can read the dataset using the Lance API:\ndataset = lance.dataset(data_uri)\nassert len(dataset.get_fragments()) == 2\nassert dataset.version == 1\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>     a  b\n0  1  x\n1  2  y\n2  3  z\n3  4  u\n4  5  v\n5  6  w\n</code></pre></p>"},{"location":"guide/distributed_write/#append-data","title":"Append data","text":"<p>Appending additional data follows a similar process. Use <code>lance.LanceOperation.Append</code> to commit the new fragments, ensuring that the <code>read_version</code> is set to the current dataset's version.</p> <pre><code>import lance\n\nds = lance.dataset(data_uri)\nread_version = ds.version # record the read version\n\nop = lance.LanceOperation.Append(schema, all_fragments)\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n</code></pre>"},{"location":"guide/distributed_write/#add-new-columns","title":"Add New Columns","text":"<p>Lance Format excels at operations such as adding columns. Thanks to its two-dimensional layout (see this blog post), adding new columns is highly efficient since it avoids copying the existing data files. Instead, the process simply creates new data files and links them to the existing dataset using metadata-only operations.</p> <pre><code>import lance\nfrom pyarrow import RecordBatch\nimport pyarrow.compute as pc\n\ndataset = lance.dataset(\"./add_columns_example\")\nassert len(dataset.get_fragments()) == 2\nassert dataset.to_table().combine_chunks() == pa.Table.from_pydict({\n    \"name\": [\"alice\", \"bob\", \"charlie\", \"craig\", \"dave\", \"eve\"],\n    \"age\": [25, 33, 44, 55, 66, 77],\n}, schema=schema)\n\n\ndef name_len(names: RecordBatch) -&gt; RecordBatch:\n    return RecordBatch.from_arrays(\n        [pc.utf8_length(names[\"name\"])],\n        [\"name_len\"],\n    )\n\n# On Worker 1\nfrag1 = dataset.get_fragments()[0]\nnew_fragment1, new_schema = frag1.merge_columns(name_len, [\"name\"])\n\n# On Worker 2\nfrag2 = dataset.get_fragments()[1]\nnew_fragment2, _ = frag2.merge_columns(name_len, [\"name\"])\n\n# On Worker 3 - Commit\nall_fragments = [new_fragment1, new_fragment2]\nop = lance.LanceOperation.Merge(all_fragments, schema=new_schema)\nlance.LanceDataset.commit(\n    \"./add_columns_example\",\n    op,\n    read_version=dataset.version,\n)\n\n# Verify dataset\ndataset = lance.dataset(\"./add_columns_example\")\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>      name  age  name_len\n0    alice   25         5\n1      bob   33         3\n2  charlie   44         7\n3    craig   55         5\n4     dave   66         4\n5      eve   77         3\n</code></pre></p>"},{"location":"guide/object_store/","title":"Object Store Configuration","text":"<p>Lance supports object stores such as AWS S3 (and compatible stores), Azure Blob Store, and Google Cloud Storage. Which object store to use is determined by the URI scheme of the dataset path. For example, <code>s3://bucket/path</code> will use S3, <code>az://bucket/path</code> will use Azure, and <code>gs://bucket/path</code> will use GCS.</p> <p>These object stores take additional configuration objects. There are two ways to specify these configurations: by setting environment variables or by passing them to the <code>storage_options</code> parameter of <code>lance.dataset</code> and <code>lance.write_dataset</code>. So for example, to globally set a higher timeout, you would run in your shell:</p> <pre><code>export TIMEOUT=60s\n</code></pre> <p>If you only want to set the timeout for a single dataset, you can pass it as a storage option:</p> <pre><code>import lance\nds = lance.dataset(\"s3://path\", storage_options={\"timeout\": \"60s\"})\n</code></pre>"},{"location":"guide/object_store/#general-configuration","title":"General Configuration","text":"<p>These options apply to all object stores.</p> Key Description <code>allow_http</code> Allow non-TLS, i.e. non-HTTPS connections. Default, <code>False</code>. <code>download_retry_count</code> Number of times to retry a download. Default, <code>3</code>. This limit is applied when the HTTP request succeeds but the response is not fully downloaded, typically due to a violation of <code>request_timeout</code>. <code>allow_invalid_certificates</code> Skip certificate validation on https connections. Default, <code>False</code>. Warning: This is insecure and should only be used for testing. <code>connect_timeout</code> Timeout for only the connect phase of a Client. Default, <code>5s</code>. <code>request_timeout</code> Timeout for the entire request, from connection until the response body has finished. Default, <code>30s</code>. <code>user_agent</code> User agent string to use in requests. <code>proxy_url</code> URL of a proxy server to use for requests. Default, <code>None</code>. <code>proxy_ca_certificate</code> PEM-formatted CA certificate for proxy connections <code>proxy_excludes</code> List of hosts that bypass proxy. This is a comma separated list of domains and IP masks. Any subdomain of the provided domain will be bypassed. For example, <code>example.com, 192.168.1.0/24</code> would bypass <code>https://api.example.com</code>, <code>https://www.example.com</code>, and any IP in the range <code>192.168.1.0/24</code>. <code>client_max_retries</code> Number of times for a s3 client to retry the request. Default, <code>10</code>. <code>client_retry_timeout</code> Timeout for a s3 client to retry the request in seconds. Default, <code>180</code>."},{"location":"guide/object_store/#s3-configuration","title":"S3 Configuration","text":"<p>S3 (and S3-compatible stores) have additional configuration options that configure authorization and S3-specific features (such as server-side encryption).</p> <p>AWS credentials can be set in the environment variables <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>. Alternatively, they can be passed as parameters to the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"access_key_id\": \"my-access-key\",\n        \"secret_access_key\": \"my-secret-key\",\n        \"session_token\": \"my-session-token\",\n    }\n)\n</code></pre> <p>If you are using AWS SSO, you can specify the <code>AWS_PROFILE</code> environment variable. It cannot be specified in the <code>storage_options</code> parameter.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>aws_region</code> / <code>region</code> The AWS region the bucket is in. This can be automatically detected when using AWS S3, but must be specified for S3-compatible stores. <code>aws_access_key_id</code> / <code>access_key_id</code> The AWS access key ID to use. <code>aws_secret_access_key</code> / <code>secret_access_key</code> The AWS secret access key to use. <code>aws_session_token</code> / <code>session_token</code> The AWS session token to use. <code>aws_endpoint</code> / <code>endpoint</code> The endpoint to use for S3-compatible stores. <code>aws_virtual_hosted_style_request</code> / <code>virtual_hosted_style_request</code> Whether to use virtual hosted-style requests, where bucket name is part of the endpoint. Meant to be used with <code>aws_endpoint</code>. Default, <code>False</code>. <code>aws_s3_express</code> / <code>s3_express</code> Whether to use S3 Express One Zone endpoints. Default, <code>False</code>. See more details below. <code>aws_server_side_encryption</code> The server-side encryption algorithm to use. Must be one of <code>\"AES256\"</code>, <code>\"aws:kms\"</code>, or <code>\"aws:kms:dsse\"</code>. Default, <code>None</code>. <code>aws_sse_kms_key_id</code> The KMS key ID to use for server-side encryption. If set, <code>aws_server_side_encryption</code> must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>. <code>aws_sse_bucket_key_enabled</code> Whether to use bucket keys for server-side encryption."},{"location":"guide/object_store/#s3-compatible-stores","title":"S3-compatible stores","text":"<p>Lance can also connect to S3-compatible stores, such as MinIO. To do so, you must specify both region and endpoint:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)\n</code></pre> <p>This can also be done with the <code>AWS_ENDPOINT</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p>"},{"location":"guide/object_store/#s3-express-directory-bucket","title":"S3 Express (Directory Bucket)","text":"<p>Lance supports S3 Express One Zone buckets, a.k.a. S3 directory buckets. S3 Express buckets only support connecting from an EC2 instance within the same region. By default, Lance automatically recognize the <code>--x-s3</code> suffix of an express bucket, there is no special configuration needed.</p> <p>In case of an access point or private link that hides the bucket name, you can configure express bucket access explicitly through storage option <code>s3_express</code>.</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://my-bucket--use1-az4--x-s3/path/imagenet.lance\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)\n</code></pre>"},{"location":"guide/object_store/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>GCS credentials are configured by setting the <code>GOOGLE_SERVICE_ACCOUNT</code> environment variable to the path of a JSON file containing the service account credentials. Alternatively, you can pass the path to the JSON file in the <code>storage_options</code></p> <pre><code>import lance\nds = lance.dataset(\n    \"gs://my-bucket/my-dataset\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)\n</code></pre> <p>Note</p> <p>By default, GCS uses HTTP/1 for communication, as opposed to HTTP/2. This improves maximum throughput significantly. However, if you wish to use HTTP/2 for some reason, you can set the environment variable <code>HTTP1_ONLY</code> to <code>false</code>.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>google_service_account</code> / <code>service_account</code> Path to the service account JSON file. <code>google_service_account_key</code> / <code>service_account_key</code> The serialized service account key. <code>google_application_credentials</code> / <code>application_credentials</code> Path to the application credentials."},{"location":"guide/object_store/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Azure Blob Storage credentials can be configured by setting the <code>AZURE_STORAGE_ACCOUNT_NAME</code> and <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variables. Alternatively, you can pass the account name and key in the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"az://my-container/my-dataset\",\n    storage_options={\n        \"account_name\": \"some-account\",\n        \"account_key\": \"some-key\",\n    }\n)\n</code></pre> <p>These keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>azure_storage_account_name</code> / <code>account_name</code> The name of the azure storage account. <code>azure_storage_account_key</code> / <code>account_key</code> The serialized service account key. <code>azure_client_id</code> / <code>client_id</code> Service principal client id for authorizing requests. <code>azure_client_secret</code> / <code>client_secret</code> Service principal client secret for authorizing requests. <code>azure_tenant_id</code> / <code>tenant_id</code> Tenant id used in oauth flows. <code>azure_storage_sas_key</code> / <code>azure_storage_sas_token</code> / <code>sas_key</code> / <code>sas_token</code> Shared access signature. The signature is expected to be percent-encoded, much like they are provided in the azure storage explorer or azure portal. <code>azure_storage_token</code> / <code>bearer_token</code> / <code>token</code> Bearer token. <code>azure_storage_use_emulator</code> / <code>object_store_use_emulator</code> / <code>use_emulator</code> Use object store with azurite storage emulator. <code>azure_endpoint</code> / <code>endpoint</code> Override the endpoint used to communicate with blob storage. <code>azure_use_fabric_endpoint</code> / <code>use_fabric_endpoint</code> Use object store with url scheme account.dfs.fabric.microsoft.com. <code>azure_msi_endpoint</code> / <code>azure_identity_endpoint</code> / <code>identity_endpoint</code> / <code>msi_endpoint</code> Endpoint to request a imds managed identity token. <code>azure_object_id</code> / <code>object_id</code> Object id for use with managed identity authentication. <code>azure_msi_resource_id</code> / <code>msi_resource_id</code> Msi resource id for use with managed identity authentication. <code>azure_federated_token_file</code> / <code>federated_token_file</code> File containing token for Azure AD workload identity federation. <code>azure_use_azure_cli</code> / <code>use_azure_cli</code> Use azure cli for acquiring access token. <code>azure_disable_tagging</code> / <code>disable_tagging</code> Disables tagging objects. This can be desirable if not supported by the backing store."},{"location":"guide/performance/","title":"Lance Performance Guide","text":"<p>This guide provides tips and tricks for optimizing the performance of your Lance applications.</p>"},{"location":"guide/performance/#trace-events","title":"Trace Events","text":"<p>Lance uses tracing to log events. If you are running <code>pylance</code> then these events will be emitted to as log messages. For Rust connections you can use the <code>tracing</code> crate to capture these events.</p>"},{"location":"guide/performance/#file-audit","title":"File Audit","text":"<p>File audit events are emitted when significant files are created or deleted.</p> Event Parameter Description <code>lance::file_audit</code> <code>mode</code> The mode of I/O operation (create, delete, delete_unverified) <code>lance::file_audit</code> <code>type</code> The type of file affected (manifest, data file, index file, deletion file)"},{"location":"guide/performance/#io-events","title":"I/O Events","text":"<p>I/O events are emitted when significant I/O operations are performed, particularly those related to indices. These events are NOT emitted when the index is loaded from the in-memory cache. Correct cache utilization is important for performance and these events are intended to help you debug cache usage.</p> Event Parameter Description <code>lance::io_events</code> <code>type</code> The type of I/O operation (open_scalar_index, open_vector_index, load_vector_part, load_scalar_part)"},{"location":"guide/performance/#execution-events","title":"Execution Events","text":"<p>Execution events are emitted when an execution plan is run. These events are useful for debugging query performance.</p> Event Parameter Description <code>lance::execution</code> <code>type</code> The type of execution event (plan_run is the only type today) <code>lance::execution</code> <code>output_rows</code> The number of rows in the output of the plan <code>lance::execution</code> <code>iops</code> The number of I/O operations performed by the plan <code>lance::execution</code> <code>bytes_read</code> The number of bytes read by the plan <code>lance::execution</code> <code>indices_loaded</code> The number of indices loaded by the plan <code>lance::execution</code> <code>parts_loaded</code> The number of index partitions loaded by the plan <code>lance::execution</code> <code>index_comparisons</code> The number of comparisons performed inside the various indices"},{"location":"guide/performance/#threading-model","title":"Threading Model","text":"<p>Lance is designed to be thread-safe and performant. Lance APIs can be called concurrently unless explicitly stated otherwise. Users may create multiple tables and share tables between threads. Operations may run in parallel on the same table, but some operations may lead to conflicts. For details see conflict resolution.</p> <p>Most Lance operations will use multiple threads to perform work in parallel. There are two thread pools in lance: the IO thread pool and the compute thread pool. The IO thread pool is used for reading and writing data from disk. The compute thread pool is used for performing computations on data. The number of threads in each pool can be configured by the user.</p> <p>The IO thread pool is used for reading and writing data from disk. The number of threads in the IO thread pool is determined by the object store that the operation is working with. Local object stores will use 8 threads by default. Cloud object stores will use 64 threads by default. This is a fairly conservative default and you may need 128 or 256 threads to saturate network bandwidth on some cloud providers. The <code>LANCE_IO_THREADS</code> environment variable can be used to override the number of IO threads. If you increase this variable you may also want to increase the <code>io_buffer_size</code>.</p> <p>The compute thread pool is used for performing computations on data. The number of threads in the compute thread pool is determined by the number of cores on the machine. The number of threads in the compute thread pool can be overridden by setting the <code>LANCE_CPU_THREADS</code> environment variable. This is commonly done when running multiple Lance processes on the same machine (e.g when working with tools like Ray). Keep in mind that decoding data is a compute intensive operation, even if a workload seems I/O bound (like scanning a table) it may still need quite a few compute threads to achieve peak performance.</p>"},{"location":"guide/performance/#memory-requirements","title":"Memory Requirements","text":"<p>Lance is designed to be memory efficient. Operations should stream data from disk and not require loading the entire dataset into memory. However, there are a few components of Lance that can use a lot of memory.</p>"},{"location":"guide/performance/#index-cache","title":"Index Cache","text":"<p>Lance uses an index cache to speed up queries. This caches vector and scalar indices in memory. The max size of this cache can be configured when creating a <code>LanceDataset</code> using the <code>index_cache_size</code> parameter. This cache is an LRU cached that is sized by \"number of entries\". The size of each entry and the number of entries needed depends on the index in question. For example, an IVF/PQ vector index contains 1 header entry and 1 entry for each partition. The size of each entry is determined by the number of vectors and the PQ parameters (e.g. number of subvectors). You can view the size of this cache by inspecting the result of <code>dataset.session().size_bytes()</code>.</p> <p>The index cache is not shared between tables. For best performance you should create a single table and share it across your application.</p>"},{"location":"guide/performance/#scanning-data","title":"Scanning Data","text":"<p>Searches (e.g. vector search, full text search) do not use a lot of memory to hold data because they don't typically return a lot of data. However, scanning data can use a lot of memory. Scanning is a streaming operation but we need enough memory to hold the data that we are scanning. The amount of memory needed is largely determined by the <code>io_buffer_size</code> and the <code>batch_size</code> variables.</p> <p>Each I/O thread should have enough memory to buffer an entire page of data. Pages today are typically between 8 and 32 MB. This means, as a rule of thumb, you should generally have about 32MB of memory per I/O thread. The default <code>io_buffer_size</code> is 2GB which is enough to buffer 64 pages of data. If you increase the number of I/O threads you should also increase the <code>io_buffer_size</code>.</p> <p>Scans will also decode data (and run any filtering or compute) in parallel on CPU threads. The amount of data decoded at any one time is determined by the <code>batch_size</code> and the size of your rows. Each CPU thread will need enough memory to hold one batch. Once batches are delivered to your application, they are no longer tracked by Lance and so if memory is a concern then you should also be careful not to accumulate memory in your own application (e.g. by running <code>to_table</code> or otherwise collecting all batches in memory.)</p> <p>The default <code>batch_size</code> is 8192 rows. When you are working with mostly scalar data you want to keep batches around 1MB and so the amount of memory needed by the compute threads is fairly small. However, when working with large data you may need to turn down the <code>batch_size</code> to keep memory usage under control. For example, when working with 1024-dimensional vector embeddings (e.g. 32-bit floats) then 8192 rows would be 32MB of data. If you spread that across 16 CPU threads then you would need 512MB of compute memory per scan. You might find working with 1024 rows per batch is more appropriate.</p> <p>In summary, scans could use up to <code>(2 * io_buffer_size) + (batch_size * num_compute_threads)</code> bytes of memory. Keep in mind that <code>io_buffer_size</code> is a soft limit (e.g. we cannot read less than one page at a time right now) and so it is not necessarily a bug if you see memory usage exceed this limit by a small margin.</p> <p>The above limits refer to limits per-scan. There is an additional limit on the number of IOPS that is applied across the entire process. This limit is specified by the <code>LANCE_PROCESS_IO_THREADS_LIMIT</code> environment variable. The default is 128 which is more than enough for most workloads. You can increase this limit if you are working with a high-throughput workload. You can even disable this limit entirely by setting it to zero. Note that this can often lead to issues with excessive retries and timeouts from the object store. </p>"},{"location":"guide/read_and_write/","title":"Read and Write Data","text":""},{"location":"guide/read_and_write/#writing-lance-dataset","title":"Writing Lance Dataset","text":"<p>If you're familiar with Apache PyArrow, you'll find that creating a Lance dataset is straightforward. Begin by writing a <code>pyarrow.Table</code> using the <code>lance.write_dataset</code> function.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./alice_and_bob.lance\")\n</code></pre> <p>If the dataset is too large to fully load into memory, you can stream data using <code>lance.write_dataset</code> also supports <code>Iterator</code> of <code>pyarrow.RecordBatch</code> es. You will need to provide a <code>pyarrow.Schema</code> for the dataset in this case.</p> <pre><code>def producer() -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"An iterator of RecordBatches.\"\"\"\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Alice\", \"age\": 20}])\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Bob\", \"age\": 30}])\n\nschema = pa.schema([\n    (\"name\", pa.string()),\n    (\"age\", pa.int32()),\n])\n\nds = lance.write_dataset(producer(),\n                         \"./alice_and_bob.lance\",\n                         schema=schema, mode=\"overwrite\")\nprint(ds.count_rows())  # Output: 2\n</code></pre> <p><code>lance.write_dataset</code> supports writing <code>pyarrow.Table</code>, <code>pandas.DataFrame</code>, <code>pyarrow.dataset.Dataset</code>, and <code>Iterator[pyarrow.RecordBatch]</code>.</p>"},{"location":"guide/read_and_write/#adding-rows","title":"Adding Rows","text":"<p>To insert data into your dataset, you can use either <code>LanceDataset.insert</code> or <code>lance.write_dataset</code> with <code>mode=append</code>.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./insert_example.lance\")\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 37}])\nds.insert(new_table)\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n\nnew_table2 = pa.Table.from_pylist([{\"name\": \"David\", \"age\": 42}])\nds = lance.write_dataset(new_table2, ds, mode=\"append\")\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#deleting-rows","title":"Deleting rows","text":"<p>Lance supports deleting rows from a dataset using a SQL filter, as described in Filter push-down. For example, to delete Bob's row from the dataset above, one could use:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.delete(\"name = 'Bob'\")\ndataset2 = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset2.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n</code></pre> <p>Note</p> <p>Lance Format is immutable. Each write operation creates a new version of the dataset, so users must reopen the dataset to see the changes. Likewise, rows are removed by marking them as deleted in a separate deletion index, rather than rewriting the files. This approach is faster and avoids invalidating any indices that reference the files, ensuring that subsequent queries do not return the deleted rows.</p>"},{"location":"guide/read_and_write/#updating-rows","title":"Updating rows","text":"<p>Lance supports updating rows based on SQL expressions with the <code>lance.LanceDataset.update</code> method. For example, if we notice that Bob's name in our dataset has been sometimes written as <code>Blob</code>, we can fix that with:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"name\": \"'Bob'\"}, where=\"name = 'Blob'\")\n</code></pre> <p>The update values are SQL expressions, which is why <code>'Bob'</code> is wrapped in single quotes. This means we can use complex expressions that reference existing columns if we wish. For example, if two years have passed and we wish to update the ages of Alice and Bob in the same example, we could write:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"age\": \"age + 2\"})\n</code></pre> <p>If you are trying to update a set of individual rows with new values then it is often more efficient to use the merge insert operation described below.</p> <pre><code>import lance\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 30},\n                                  {\"name\": \"Bob\", \"age\": 20}])\n\n# This works, but is inefficient, see below for a better approach\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nfor idx in range(new_table.num_rows):\n  name = new_table[0][idx].as_py()\n  new_age = new_table[1][idx].as_py()\n  dataset.update({\"age\": new_age}, where=f\"name='{name}'\")\n</code></pre>"},{"location":"guide/read_and_write/#merge-insert","title":"Merge Insert","text":"<p>Lance supports a merge insert operation. This can be used to add new data in bulk while also (potentially) matching against existing data. This operation can be used for a number of different use cases.</p>"},{"location":"guide/read_and_write/#bulk-update","title":"Bulk Update","text":"<p>The <code>lance.LanceDataset.update</code> method is useful for updating rows based on a filter. However, if we want to replace existing rows with new rows then a <code>lance.LanceDataset.merge_insert</code> operation would be more efficient:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 2},\n                                  {\"name\": \"Bob\", \"age\": 3}])\n# This will use `name` as the key for matching rows.  Merge insert\n# uses a JOIN internally and so you typically want this column to\n# be a unique key or id of some kind.\nrst = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .execute(new_table)\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice    2\n# 1    Bob    3\n</code></pre> <p>Note that, similar to the update operation, rows that are modified will be removed and inserted back into the table, changing their position to the end. Also, the relative order of these rows could change because we are using a hash-join operation internally.</p>"},{"location":"guide/read_and_write/#insert-if-not-exists","title":"Insert if not Exists","text":"<p>Sometimes we only want to insert data if we haven't already inserted it before. This can happen, for example, when we have a batch of data but we don't know which rows we've added previously and we don't want to create duplicate rows. We can use the merge insert operation to achieve this:</p> <pre><code># Bob is already in the table, but Carla is new\nnew_table = pa.Table.from_pylist([{\"name\": \"Bob\", \"age\": 30},\n                                  {\"name\": \"Carla\", \"age\": 37}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will insert Carla but leave Bob unchanged\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify that Carla was added but Bob remains unchanged\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n</code></pre>"},{"location":"guide/read_and_write/#update-or-insert-upsert","title":"Update or Insert (Upsert)","text":"<p>Sometimes we want to combine both of the above behaviors. If a row already exists we want to update it. If the row does not exist we want to add it. This operation is sometimes called \"upsert\". We can use the merge insert operation to do this as well:</p> <pre><code>import lance\nimport pyarrow as pa\n\n# Change Carla's age and insert David\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 27},\n                                  {\"name\": \"David\", \"age\": 42}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will update Carla and insert David\n_ = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify the results\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   27\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#replace-a-portion-of-data","title":"Replace a Portion of Data","text":"<p>A less common, but still useful, behavior can be to replace some region of existing rows (defined by a filter) with new data. This is similar to performing both a delete and an insert in a single transaction. For example:</p> <pre><code>import lance\nimport pyarrow as pa\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Edgar\", \"age\": 46},\n                                  {\"name\": \"Francene\", \"age\": 44}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#       name  age\n# 0    Alice   20\n# 1      Bob   30\n# 2  Charlie   45\n# 3    Donna   50\n\n# This will remove anyone above 40 and insert our new data\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .when_not_matched_by_source_delete(\"age &gt;= 40\") \\\n       .execute(new_table)\n# Verify the results - people over 40 replaced with new data\nprint(dataset.to_table().to_pandas())\n#        name  age\n# 0     Alice   20\n# 1       Bob   30\n# 2     Edgar   46\n# 3  Francene   44\n</code></pre>"},{"location":"guide/read_and_write/#reading-lance-dataset","title":"Reading Lance Dataset","text":"<p>To open a Lance dataset, use the <code>lance.dataset</code> function:</p> <pre><code>import lance\nds = lance.dataset(\"s3://bucket/path/imagenet.lance\")\n# Or local path\nds = lance.dataset(\"./imagenet.lance\")\n</code></pre> <p>Note</p> <p>Lance supports local file system, AWS <code>s3</code> and Google Cloud Storage(<code>gs</code>) as storage backends at the moment. Read more in Object Store Configuration.</p> <p>The most straightforward approach for reading a Lance dataset is to utilize the <code>lance.LanceDataset.to_table</code> method in order to load the entire dataset into memory.</p> <pre><code>table = ds.to_table()\n</code></pre> <p>Due to Lance being a high-performance columnar format, it enables efficient reading of subsets of the dataset by utilizing Column (projection) push-down and filter (predicates) push-downs.</p> <pre><code>table = ds.to_table(\n    columns=[\"image\", \"label\"],\n    filter=\"label = 2 AND text IS NOT NULL\",\n    limit=1000,\n    offset=3000)\n</code></pre> <p>Lance understands the cost of reading heavy columns such as <code>image</code>. Consequently, it employs an optimized query plan to execute the operation efficiently.</p>"},{"location":"guide/read_and_write/#iterative-read","title":"Iterative Read","text":"<p>If the dataset is too large to fit in memory, you can read it in batches using the <code>lance.LanceDataset.to_batches</code> method:</p> <pre><code>for batch in ds.to_batches(columns=[\"image\"], filter=\"label = 10\"):\n    # do something with batch\n    compute_on_batch(batch)\n</code></pre> <p>Unsurprisingly, <code>lance.LanceDataset.to_batches</code> takes the same parameters as <code>lance.LanceDataset.to_table</code> function.</p>"},{"location":"guide/read_and_write/#filter-push-down","title":"Filter push-down","text":"<p>Lance embraces the utilization of standard SQL expressions as predicates for dataset filtering. By pushing down the SQL predicates directly to the storage system, the overall I/O load during a scan is significantly reduced.</p> <p>Currently, Lance supports a growing list of expressions.</p> <ul> <li><code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>=</code></li> <li><code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li><code>IS NULL</code>, <code>IS NOT NULL</code></li> <li><code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code></li> <li><code>IN</code></li> <li><code>LIKE</code>, <code>NOT LIKE</code></li> <li><code>regexp_match(column, pattern)</code></li> <li><code>CAST</code></li> </ul> <p>For example, the following filter string is acceptable:</p> <pre><code>((label IN [10, 20]) AND (note['email'] IS NOT NULL))\n    OR NOT note['created']\n</code></pre> <p>Nested fields can be accessed using the subscripts. Struct fields can be subscripted using field names, while list fields can be subscripted using indices.</p> <p>If your column name contains special characters or is a SQL Keyword, you can use backtick (<code>`</code>) to escape it. For nested fields, each segment of the path must be wrapped in backticks.</p> <pre><code>`CUBE` = 10 AND `column name with space` IS NOT NULL\n  AND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Warning</p> <p>Field names containing periods (<code>.</code>) are not supported.</p> <p>Literals for dates, timestamps, and decimals can be written by writing the string value after the type name. For example</p> <pre><code>date_col = date '2021-01-01'\nand timestamp_col = timestamp '2021-01-01 00:00:00'\nand decimal_col = decimal(8,3) '1.000'\n</code></pre> <p>For timestamp columns, the precision can be specified as a number in the type parameter. Microsecond precision (6) is the default.</p> SQL Time unit <code>timestamp(0)</code> Seconds <code>timestamp(3)</code> Milliseconds <code>timestamp(6)</code> Microseconds <code>timestamp(9)</code> Nanoseconds <p>Lance internally stores data in Arrow format. The mapping from SQL types to Arrow is:</p> SQL type Arrow type <code>boolean</code> <code>Boolean</code> <code>tinyint</code> / <code>tinyint unsigned</code> <code>Int8</code> / <code>UInt8</code> <code>smallint</code> / <code>smallint unsigned</code> <code>Int16</code> / <code>UInt16</code> <code>int</code> or <code>integer</code> / <code>int unsigned</code> or <code>integer unsigned</code> <code>Int32</code> / <code>UInt32</code> <code>bigint</code> / <code>bigint unsigned</code> <code>Int64</code> / <code>UInt64</code> <code>float</code> <code>Float32</code> <code>double</code> <code>Float64</code> <code>decimal(precision, scale)</code> <code>Decimal128</code> <code>date</code> <code>Date32</code> <code>timestamp</code> <code>Timestamp</code> (1) <code>string</code> <code>Utf8</code> <code>binary</code> <code>Binary</code> <p>(1) See precision mapping in previous table.</p>"},{"location":"guide/read_and_write/#random-read","title":"Random read","text":"<p>One distinct feature of Lance, as columnar format, is that it allows you to read random samples quickly.</p> <pre><code># Access the 2nd, 101th and 501th rows\ndata = ds.take([1, 100, 500], columns=[\"image\", \"label\"])\n</code></pre> <p>The ability to achieve fast random access to individual rows plays a crucial role in facilitating various workflows such as random sampling and shuffling in ML training. Additionally, it empowers users to construct secondary indices, enabling swift execution of queries for enhanced performance.</p>"},{"location":"guide/read_and_write/#table-maintenance","title":"Table Maintenance","text":"<p>Some operations over time will cause a Lance dataset to have a poor layout. For example, many small appends will lead to a large number of small fragments. Or deleting many rows will lead to slower queries due to the need to filter out deleted rows.</p> <p>To address this, Lance provides methods for optimizing dataset layout.</p>"},{"location":"guide/read_and_write/#compact-data-files","title":"Compact data files","text":"<p>Data files can be rewritten so there are fewer files. When passing a <code>target_rows_per_fragment</code> to <code>lance.dataset.DatasetOptimizer.compact_files</code>, Lance will skip any fragments that are already above that row count, and rewrite others. Fragments will be merged according to their fragment ids, so the inherent ordering of the data will be preserved.</p> <p>Note</p> <p>Compaction creates a new version of the table. It does not delete the old version of the table and the files referenced by it.</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.optimize.compact_files(target_rows_per_fragment=1024 * 1024)\n</code></pre> <p>During compaction, Lance can also remove deleted rows. Rewritten fragments will not have deletion files. This can improve scan performance since the soft deleted rows don't have to be skipped during the scan.</p> <p>When files are rewritten, the original row addresses are invalidated. This means the affected files are no longer part of any ANN index if they were before. Because of this, it's recommended to rewrite files before re-building indices.</p>"},{"location":"guide/tags/","title":"Manage Tags","text":"<p>Lance, much like Git, employs the <code>LanceDataset.tags</code> property to label specific versions within a dataset's history.</p> <p><code>Tags</code> are particularly useful for tracking the evolution of datasets, especially in machine learning workflows where datasets are frequently updated. For example, you can <code>create</code>, <code>update</code>, and <code>delete</code> or <code>list</code> tags.</p> <p>Note</p> <p>Creating or deleting tags does not generate new dataset versions. Tags exist as auxiliary metadata stored in a separate directory.</p> <pre><code>import lance\nds = lance.dataset(\"./tags.lance\")\nprint(len(ds.versions()))\n# 2\nprint(ds.tags.list())\n# {}\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 1, 'manifest_size': ...}}\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 2, 'manifest_size': ...}}\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list())\n# {}\nprint(ds.tags.list_ordered())\n# []\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 1, 'manifest_size': ...})]\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 2, 'manifest_size': ...})]\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list_ordered())\n# []\n</code></pre> <p>Note</p> <p>Tagged versions are exempted from the <code>LanceDataset.cleanup_old_versions()</code> process.</p> <p>To remove a version that has been tagged, you must first <code>LanceDataset.tags.delete()</code> the associated tag. </p>"},{"location":"guide/tokenizer/","title":"Tokenizers","text":"<p>Currently, Lance has built-in support for Jieba and Lindera. However, it doesn't come with its own language models. If tokenization is needed, you can download language models by yourself. You can specify the location where the language models are stored by setting the environment variable LANCE_LANGUAGE_MODEL_HOME. If it's not set, the default value is</p> <pre><code>${system data directory}/lance/language_models\n</code></pre> <p>It also supports configuring user dictionaries, which makes it convenient for users to expand their own dictionaries without retraining the language models.</p>"},{"location":"guide/tokenizer/#language-models-of-jieba","title":"Language Models of Jieba","text":""},{"location":"guide/tokenizer/#downloading-the-model","title":"Downloading the Model","text":"<pre><code>python -m lance.download jieba\n</code></pre> <p>The language model is stored by default in <code>${LANCE_LANGUAGE_MODEL_HOME}/jieba/default</code>.</p>"},{"location":"guide/tokenizer/#using-the-model","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"jieba/default\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries","title":"User Dictionaries","text":"<p>Create a file named config.json in the root directory of the current model.</p> <pre><code>{\n    \"main\": \"dict.txt\",\n    \"users\": [\"path/to/user/dict.txt\"]\n}\n</code></pre> <ul> <li>The \"main\" field is optional. If not filled, the default is \"dict.txt\".</li> <li>\"users\" is the path of the user dictionary. For the format of the user dictionary, please refer to https://github.com/messense/jieba-rs/blob/main/src/data/dict.txt.</li> </ul>"},{"location":"guide/tokenizer/#language-models-of-lindera","title":"Language Models of Lindera","text":""},{"location":"guide/tokenizer/#downloading-the-model_1","title":"Downloading the Model","text":"<pre><code>python -m lance.download lindera -l [ipadic|ko-dic|unidic]\n</code></pre> <p>Note that the language models of Lindera need to be compiled. Please install lindera-cli first. For detailed steps, please refer to https://github.com/lindera/lindera/tree/main/lindera-cli.</p> <p>The language model is stored by default in ${LANCE_LANGUAGE_MODEL_HOME}/lindera/[ipadic|ko-dic|unidic]</p>"},{"location":"guide/tokenizer/#using-the-model_1","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"lindera/ipadic\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries_1","title":"User Dictionaries","text":"<p>Create a file named config.yml in the root directory of your model, or specify a custom YAML file using the <code>LINDERA_CONFIG_PATH</code> environment variable. If both are provided, the config.yml in the root directory will be used. For more detailed configuration methods, see the lindera documentation at https://github.com/lindera/lindera/.</p> <pre><code>segmenter:\n    mode: \"normal\"\n    dictionary:\n        # Note: in lance, the `kind` field is not supported. You need to specify the model path using the `path` field instead.\n        path: /path/to/lindera/ipadic/main\n</code></pre>"},{"location":"guide/tokenizer/#create-your-own-language-model","title":"Create your own language model","text":"<p>Put your language model into <code>LANCE_LANGUAGE_MODEL_HOME</code>. </p>"},{"location":"integrations/datafusion/","title":"Apache DataFusion Integration","text":"<p>Lance datasets can be queried with Apache Datafusion,  an extensible query engine written in Rust that uses Apache Arrow as its in-memory format.  This means you can write complex SQL queries to analyze your data in Lance.</p> <p>The integration allows users to pass down column selections and basic filters to Lance,  reducing the amount of scanned data when executing your query.  Additionally, the integration allows streaming data from Lance datasets, which allows users to do aggregation larger-than-memory.</p>"},{"location":"integrations/datafusion/#rust","title":"Rust","text":"<p>Lance includes a DataFusion table provider <code>lance::datafusion::LanceTableProvider</code>. Users can register a Lance dataset as a table in DataFusion and run SQL with it:</p>"},{"location":"integrations/datafusion/#simple-sql","title":"Simple SQL","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse crate::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"dataset\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"SELECT * FROM dataset LIMIT 10\").await?;\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#join-2-tables","title":"Join 2 Tables","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse crate::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"orders\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(orders_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nctx.register_table(\"customers\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(customers_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"\n    SELECT o.order_id, o.amount, c.customer_name \n    FROM orders o \n    JOIN customers c ON o.customer_id = c.customer_id\n    LIMIT 10\n\").await?;\n\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#python","title":"Python","text":"<p>In Python, this integration is done via Datafusion FFI. An FFI table provider <code>FFILanceTableProvider</code> is included in <code>pylance</code>. For example, if I want to query <code>my_lance_dataset</code>:</p> <pre><code>from datafusion import SessionContext # pip install datafusion\nfrom lance import FFILanceTableProvider\n\nctx = SessionContext()\n\ntable1 = FFILanceTableProvider(\n    my_lance_dataset, with_row_id=True, with_row_addr=True\n)\nctx.register_table_provider(\"table1\", table1)\nctx.table(\"table1\")\nctx.sql(\"SELECT * FROM table1 LIMIT 10\")\n</code></pre>"},{"location":"integrations/duckdb/","title":"DuckDB","text":"<p>In Python, Lance datasets can also be queried with DuckDB,  an in-process SQL OLAP database. This means you can write complex SQL queries to analyze your data in Lance.</p> <p>This integration is done via DuckDB SQL on Apache Arrow,  which provides zero-copy data sharing between LanceDB and DuckDB.  DuckDB is capable of passing down column selections and basic filters to Lance,  reducing the amount of data that needs to be scanned to perform your query.  Finally, the integration allows streaming data from Lance tables,  allowing you to aggregate tables that won't fit into memory.  All of this uses the same mechanism described in DuckDB's  blog post DuckDB quacks Arrow.</p> <p>A <code>LanceDataset</code> is accessible to DuckDB through the Arrow compatibility layer directly. To query the resulting Lance dataset in DuckDB,  all you need to do is reference the dataset by the same name in your SQL query.</p> <p>For example, if you have a Lance dataset with variable name <code>my_lance_dataset</code>:</p> <pre><code>import duckdb # pip install duckdb\n\nduckdb.query(\"SELECT * FROM my_lance_dataset\")\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502   vector    \u2502  item   \u2502 price  \u2502\n# \u2502   float[]   \u2502 varchar \u2502 double \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 [3.1, 4.1]  \u2502 foo     \u2502   10.0 \u2502\n# \u2502 [5.9, 26.5] \u2502 bar     \u2502   20.0 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nduckdb.query(\"SELECT mean(price) FROM my_lance_dataset\")\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 mean(price) \u2502\n# \u2502   double    \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502        15.0 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/huggingface/","title":"HuggingFace Integration","text":"<p>The HuggingFace Hub has become the go to place for ML practitioners to find pre-trained models and useful datasets.</p> <p>HuggingFace datasets can be written directly into Lance format by using the <code>lance.write_dataset</code> method. You can write the entire dataset or a particular split. For example:</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nlance.write_dataset(datasets.load_dataset(\n    \"poloclub/diffusiondb\", split=\"train[:10]\",\n), \"diffusiondb_train.lance\")\n</code></pre>"},{"location":"integrations/pytorch/","title":"PyTorch Integration","text":"<p>Machine learning users can use <code>lance.torch.data.LanceDataset</code>, a subclass of <code>torch.utils.data.IterableDataset</code>, that to use Lance data directly PyTorch training and inference loops.</p> <p>It starts with creating a ML dataset for training. With the HuggingFace integration, it takes just one line of Python to convert a HuggingFace dataset to a Lance dataset.</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nhf_ds = datasets.load_dataset(\n    \"poloclub/diffusiondb\",\n    split=\"train\",\n    # name=\"2m_first_1k\",  # for a smaller subset of the dataset\n)\nlance.write_dataset(hf_ds, \"diffusiondb_train.lance\")\n</code></pre> <p>Then, you can use the Lance dataset in PyTorch training and inference loops.</p> <p>Note:</p> <ol> <li> <p>the PyTorch dataset automatically convert data into <code>torch.Tensor</code></p> </li> <li> <p>lance is not fork-safe. If you are using multiprocessing, use spawn instead. The safe dataloader uses the spawning method.</p> </li> </ol>"},{"location":"integrations/pytorch/#unsafe-dataloader","title":"UnSafe Dataloader","text":"<pre><code>import torch\nimport lance.torch.data\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = lance.torch.data.LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n)\n\n# Create a PyTorch DataLoader\ndataloader = torch.utils.data.DataLoader(dataset)\n\n# Inference loop\nfor batch in dataloader:\n    inputs, targets = batch[\"prompt\"], batch[\"image\"]\n    outputs = model(inputs)\n    ...\n</code></pre>"},{"location":"integrations/pytorch/#safe-dataloader","title":"Safe Dataloader","text":"<pre><code>from lance.torch.data import SafeLanceDataset, get_safe_loader\n\ndataset = SafeLanceDataset(temp_lance_dataset)\n# use spawn method to avoid fork-safe issue\nloader = get_safe_loader(\n    dataset,\n    num_workers=2,\n    batch_size=16,\n    drop_last=False,\n)\n\ntotal_samples = 0\nfor batch in loader:\n    total_samples += batch[\"id\"].shape[0]\n</code></pre> <p><code>lance.torch.data.LanceDataset</code> can composite with the <code>lance.sampler.Sampler</code> classes to control the sampling strategy. For example, you can use <code>lance.sampler.ShardedFragmentSampler</code> to use it in a distributed training environment. If not specified, it is a full scan.</p> <pre><code>from lance.sampler import ShardedFragmentSampler\nfrom lance.torch.data import LanceDataset\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n    sampler=ShardedFragmentSampler(\n        rank=1,  # Rank of the current process\n        world_size=8,  # Total number of processes\n    ),\n)\n</code></pre> <p>Available samplers:</p> <ul> <li><code>lance.sampler.ShardedFragmentSampler</code></li> <li><code>lance.sampler.ShardedBatchSampler</code></li> </ul> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"integrations/ray/","title":"Ray Integration","text":"<p>Ray effortlessly scale up ML workload to large distributed compute environment.</p>"},{"location":"integrations/ray/#basic-operations","title":"Basic Operations","text":"<p>Lance format is one of the official Ray data sources:</p> <ul> <li>Lance Data Source <code>ray.data.read_lance</code></li> <li>Lance Data Sink <code>ray.data.Dataset.write_lance</code></li> </ul> <pre><code>import ray\nimport pandas as pd\n\nray.init()\n\ndata = [\n    {\"id\": 1, \"name\": \"alice\"},\n    {\"id\": 2, \"name\": \"bob\"},\n    {\"id\": 3, \"name\": \"charlie\"}\n]\nray.data.from_items(data).write_lance(\"./alice_bob_and_charlie.lance\")\n\n# It can be read via lance directly\ndf = (\n    lance.\n    dataset(\"./alice_bob_and_charlie.lance\")\n    .to_table()\n    .to_pandas()\n    .sort_values(by=[\"id\"])\n    .reset_index(drop=True)\n)\nassert df.equals(pd.DataFrame(data)), \"{} != {}\".format(\n    df, pd.DataFrame(data)\n)\n\n# Or via Ray.data.read_lance\nray_df = (\n    ray.data.read_lance(\"./alice_bob_and_charlie.lance\")\n    .to_pandas()\n    .sort_values(by=[\"id\"])\n    .reset_index(drop=True)\n)\nassert df.equals(ray_df)\n</code></pre>"},{"location":"integrations/ray/#advanced-operations","title":"Advanced Operations","text":""},{"location":"integrations/ray/#parallel-column-merging","title":"Parallel Column Merging","text":"<p>Demonstration of parallel column generation using Lance's native operations:</p> <pre><code>import pyarrow as pa\nfrom pathlib import Path\nimport lance\n\n# Define schema\nschema = pa.schema([\n    pa.field(\"id\", pa.int64()),\n    pa.field(\"height\", pa.int64()),\n    pa.field(\"weight\", pa.int64()),\n])\n\n# Generate initial dataset\nds = (\n    ray.data.range(10)  # Create 0-9 IDs\n    .map(lambda x: {\n        \"id\": x[\"id\"],\n        \"height\": x[\"id\"] + 5,  # height = id + 5\n        \"weight\": x[\"id\"] * 2   # weight = id * 2\n    })\n    .write_lance(str(output_path), schema=schema)\n)\n\n# Define label generation logic\ndef generate_labels(batch: pa.RecordBatch) -&gt; pa.RecordBatch:\n    heights = batch.column(\"height\").to_pylist()\n    size_labels = [\"tall\" if h &gt; 8 else \"medium\" if h &gt; 6 else \"short\" for h in heights]\n    return pa.RecordBatch.from_arrays([\n        pa.array(size_labels)\n    ], names=[\"size_labels\"])\n\n# Add new columns in parallel\nlance_ds = lance.dataset(output_path)\nadd_columns(\n    lance_ds,\n    generate_labels,\n    source_columns=[\"height\"],  # Input columns needed\n)\n\n# Display final results\nfinal_df = lance_ds.to_table().to_pandas()\nprint(\"\\nEnhanced dataset with size labels:\\n\")\nprint(final_df.sort_values(\"id\").to_string(index=False))\n</code></pre>"},{"location":"integrations/tensorflow/","title":"Tensorflow Integration","text":"<p>Lance can be used as a regular tf.data.Dataset in Tensorflow.</p> <p>Warning</p> <p>This feature is experimental and the APIs may change in the future.</p>"},{"location":"integrations/tensorflow/#reading-from-lance","title":"Reading from Lance","text":"<p>Using <code>lance.tf.data.from_lance</code>, you can create an <code>tf.data.Dataset</code> easily.</p> <pre><code>import tensorflow as tf\nimport lance\n\n# Create tf dataset\nds = lance.tf.data.from_lance(\"s3://my-bucket/my-dataset\")\n\n# Chain tf dataset with other tf primitives\n\nfor batch in ds.shuffling(32).map(lambda x: tf.io.decode_png(x[\"image\"])):\n    print(batch)\n</code></pre> <p>Backed by the Lance columnar format, using <code>lance.tf.data.from_lance</code> supports efficient column selection, filtering, and more.</p> <pre><code>ds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"label\"],\n    filter=\"split = 'train' AND collected_time &gt; timestamp '2020-01-01'\",\n    batch_size=256)\n</code></pre> <p>By default, Lance will infer the Tensor spec from the projected columns. You can also specify <code>tf.TensorSpec</code> manually.</p> <pre><code>batch_size = 256\nds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"labels\"],\n    batch_size=batch_size,\n    output_signature={\n        \"image\": tf.TensorSpec(shape=(), dtype=tf.string),\n        \"labels\": tf.RaggedTensorSpec(\n            dtype=tf.int32, shape=(batch_size, None), ragged_rank=1),\n    },\n</code></pre>"},{"location":"integrations/tensorflow/#distributed-training-and-shuffling","title":"Distributed Training and Shuffling","text":"<p>Since a Lance Dataset is a set of Fragments, we can distribute and shuffle Fragments to different workers.</p> <pre><code>import tensorflow as tf\nfrom lance.tf.data import from_lance, lance_fragments\n\nworld_size = 32\nrank = 10\nseed = 123  #\nepoch = 100\n\ndataset_uri = \"s3://my-bucket/my-dataset\"\n\n# Shuffle fragments distributedly.\nfragments =\n    lance_fragments(\"s3://my-bucket/my-dataset\")\n    .shuffling(32, seed=seed)\n    .repeat(epoch)\n    .enumerate()\n    .filter(lambda i, _: i % world_size == rank)\n    .map(lambda _, fid: fid)\n\nds = from_lance(\n    uri,\n    columns=[\"image\", \"label\"],\n    fragments=fragments,\n    batch_size=32\n    )\nfor batch in ds:\n    print(batch)\n</code></pre> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"quickstart/","title":"Getting Started with Lance Tables","text":"<p>This quickstart guide will walk you through the core features of Lance including creating datasets, versioning, and vector search.</p> <p>By the end of this tutorial, you'll be able to create Lance datasets from pandas DataFrames and convert existing Parquet files to Lance format. You'll also understand the basic workflow for working with Lance datasets and be prepared to explore advanced features like versioning and vector search.</p>"},{"location":"quickstart/#install-the-python-sdk","title":"Install the Python SDK","text":"<p>The easiest way to get started with Lance is via our Python SDK <code>pylance</code>:</p> <pre><code>pip install pylance\n</code></pre> <p>For the latest features and bug fixes, you can install the preview version:</p> <pre><code>pip install --pre --extra-index-url https://pypi.fury.io/lancedb/pylance\n</code></pre> <p>Note: Preview releases receive the same level of testing as regular releases.</p>"},{"location":"quickstart/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, let's import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/#create-your-first-dataset","title":"Create Your First Dataset","text":"<p>Lance is built on top of Apache Arrow, making it incredibly easy to work with pandas DataFrames and Arrow tables. You can create Lance datasets from various data sources including pandas DataFrames, Arrow tables, and existing Parquet files. Lance automatically handles the conversion and optimization for you.</p>"},{"location":"quickstart/#create-a-simple-dataset","title":"Create a Simple Dataset","text":"<p>You'll create a simple dataframe and then write it to Lance format. This demonstrates the basic workflow for creating Lance datasets.</p> <p>Create a simple dataframe:</p> <pre><code>df = pd.DataFrame({\"a\": [5]})\ndf\n</code></pre> <p>Now you'll write this dataframe to Lance format and verify the data was saved correctly:</p> <pre><code>shutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ndataset = lance.write_dataset(df, \"/tmp/test.lance\")\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#convert-your-existing-parquet-files","title":"Convert Your Existing Parquet Files","text":"<p>You'll convert an existing Parquet file to Lance format. This shows how to migrate your existing data to Lance.</p> <p>First, you'll create a Parquet file and then convert it to Lance:</p> <pre><code>shutil.rmtree(\"/tmp/test.parquet\", ignore_errors=True)\nshutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ntbl = pa.Table.from_pandas(df)\npa.dataset.write_dataset(tbl, \"/tmp/test.parquet\", format='parquet')\n\nparquet = pa.dataset.dataset(\"/tmp/test.parquet\")\nparquet.to_table().to_pandas()\n</code></pre> <p>Now you'll convert the Parquet dataset to Lance format in a single line:</p> <pre><code>dataset = lance.write_dataset(parquet, \"/tmp/test.lance\")\n\n# Make sure it's the same\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered the basics of creating Lance datasets, here's what you can explore next:</p> <ul> <li>Versioning Your Datasets with Lance - Learn how to track changes over time with native versioning</li> <li>Vector Indexing and Vector Search With Lance - Build high-performance vector search capabilities with ANN indexes</li> </ul>"},{"location":"quickstart/vector-search/","title":"Vector Indexing and Vector Search With Lance","text":"<p>Lance provides high-performance vector search capabilities with ANN (Approximate Nearest Neighbor) indexes.</p> <p>By the end of this tutorial, you'll be able to build and use ANN indexes to dramatically speed up vector search operations while maintaining high accuracy. You'll also learn how to tune search parameters for optimal performance and combine vector search with metadata queries in a single operation.</p>"},{"location":"quickstart/vector-search/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/vector-search/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport duckdb\n</code></pre>"},{"location":"quickstart/vector-search/#prepare-your-vector-embeddings","title":"Prepare Your Vector Embeddings","text":"<p>For this tutorial, download and prepare the SIFT 1M dataset for vector search experiments.</p> <ul> <li>Download <code>ANN_SIFT1M</code> from: http://corpus-texmex.irisa.fr/</li> <li>Direct link: <code>ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz</code></li> </ul> <p>You can just use <code>wget</code>:</p> <pre><code>rm -rf sift* vec_data.lance\nwget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\ntar -xzf sift.tar.gz\n</code></pre>"},{"location":"quickstart/vector-search/#convert-your-data-to-lance-format","title":"Convert Your Data to Lance Format","text":"<p>Then, convert the raw vector data into Lance format for efficient storage and querying.</p> <pre><code>from lance.vector import vec_to_table\nimport struct\n\nuri = \"vec_data.lance\"\n\nwith open(\"sift/sift_base.fvecs\", mode=\"rb\") as fobj:\n    buf = fobj.read()\n    data = np.array(struct.unpack(\"&lt;128000000f\", buf[4 : 4 + 4 * 1000000 * 128])).reshape((1000000, 128))\n    dd = dict(zip(range(1000000), data))\n\ntable = vec_to_table(dd)\nlance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)\n</code></pre> <p>Now you can load the dataset:</p> <pre><code>uri = \"vec_data.lance\"\nsift1m = lance.dataset(uri)\n</code></pre>"},{"location":"quickstart/vector-search/#search-without-an-index","title":"Search Without an Index","text":"<p>You'll perform vector search without an index to see the baseline performance, then compare it with indexed search.</p> <p>First, let's sample some query vectors:</p> <pre><code>import duckdb\n# Make sure DuckDB v0.7+ is installed\nsamples = duckdb.query(\"SELECT vector FROM sift1m USING SAMPLE 100\").to_df().vector\n</code></pre> <pre><code>0     [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...\n1     [7.0, 5.0, 39.0, 49.0, 17.0, 12.0, 83.0, 117.0...\n2     [0.0, 0.0, 0.0, 10.0, 12.0, 31.0, 6.0, 0.0, 0....\n3     [0.0, 2.0, 9.0, 1.793662034335766e-43, 30.0, 1...\n4     [54.0, 112.0, 16.0, 0.0, 0.0, 7.0, 112.0, 44.0...\n                            ...\n95    [1.793662034335766e-43, 33.0, 47.0, 28.0, 0.0,...\n96    [1.0, 4.0, 2.0, 32.0, 3.0, 7.0, 119.0, 116.0, ...\n97    [17.0, 46.0, 12.0, 0.0, 0.0, 3.0, 23.0, 58.0, ...\n98    [0.0, 11.0, 30.0, 14.0, 34.0, 7.0, 0.0, 0.0, 1...\n99    [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...\nName: vector, Length: 100, dtype: object\n</code></pre> <p>Now, perform nearest neighbor search without an index:</p> <pre><code>import time\n\nstart = time.time()\ntbl = sift1m.to_table(columns=[\"id\"], nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10})\nend = time.time()\n\nprint(f\"Time(sec): {end-start}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Time(sec): 0.10735273361206055\n       id                                             vector    score\n0  144678  [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...      0.0\n1  575538  [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ...  76908.0\n2  241428  [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21...  92877.0\n...\n</code></pre></p> <p>Without the index, the search will scan throughout the whole dataset to compute the distance between each data point. For practical real-time performance with, you will get much better performance with an ANN index.</p>"},{"location":"quickstart/vector-search/#build-the-search-index","title":"Build the Search Index","text":"<p>If you build an ANN index - you can dramatically speed up vector search operations while maintaining high accuracy. In this example, we will build the <code>IVF_PQ</code> index: </p> <pre><code>sift1m.create_index(\n    \"vector\",\n    index_type=\"IVF_PQ\", # specify the IVF_PQ index type\n    num_partitions=256,  # IVF\n    num_sub_vectors=16,  # PQ\n)\n</code></pre> <p>The sample response should look like this:</p> <pre><code>Building vector index: IVF256,PQ16\nCPU times: user 2min 23s, sys: 2.77 s, total: 2min 26s\nWall time: 22.7 s\nSample 65536 out of 1000000 to train kmeans of 128 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\n</code></pre> <p>Index Creation Performance</p> <p>If you're trying this on your own data, make sure your vector (dimensions / num_sub_vectors) % 8 == 0, or else index creation will take much longer than expected due to SIMD misalignment.</p>"},{"location":"quickstart/vector-search/#vector-search-with-the-ann-index","title":"Vector Search with the ANN Index","text":"<p>You can now perform the same search operation using your newly created index and see the dramatic performance improvement.</p> <pre><code>sift1m = lance.dataset(uri)\n\nimport time\n\ntot = 0\nfor q in samples:\n    start = time.time()\n    tbl = sift1m.to_table(nearest={\"column\": \"vector\", \"q\": q, \"k\": 10})\n    end = time.time()\n    tot += (end - start)\n\nprint(f\"Avg(sec): {tot / len(samples)}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Avg(sec): 0.0009334301948547364\n       id                                             vector         score\n0  378825  [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...  16560.197266\n1  143787  [11.0, 24.0, 122.0, 122.0, 53.0, 4.0, 0.0, 3.0...  61714.941406\n2  356895  [0.0, 14.0, 67.0, 122.0, 83.0, 23.0, 1.0, 0.0,...  64147.218750\n3  535431  [9.0, 22.0, 118.0, 118.0, 4.0, 5.0, 4.0, 4.0, ...  69092.593750\n4  308778  [1.0, 7.0, 48.0, 123.0, 73.0, 36.0, 8.0, 4.0, ...  69131.812500\n5  222477  [14.0, 73.0, 39.0, 4.0, 16.0, 94.0, 19.0, 8.0,...  69244.195312\n6  672558  [2.0, 1.0, 0.0, 11.0, 36.0, 23.0, 7.0, 10.0, 0...  70264.828125\n7  365538  [54.0, 43.0, 97.0, 59.0, 34.0, 17.0, 10.0, 15....  70273.710938\n8  659787  [10.0, 9.0, 23.0, 121.0, 38.0, 26.0, 38.0, 9.0...  70374.703125\n9  603930  [32.0, 32.0, 122.0, 122.0, 70.0, 4.0, 15.0, 12...  70583.375000\n</code></pre></p> <p>Performance Note</p> <p>Your actual numbers will vary by your storage. These numbers are from local disk on an M2 MacBook Air. If you're querying S3 directly, HDD, or network drives, performance will be slower.</p>"},{"location":"quickstart/vector-search/#tune-the-search-parameters","title":"Tune the Search Parameters","text":"<p>You need to adjust search parameters to balance between speed and accuracy, finding the optimal settings for your use case.</p> <p>The latency vs recall is tunable via: - nprobes: how many IVF partitions to search - refine_factor: determines how many vectors are retrieved during re-ranking</p> <pre><code>%%time\n\nsift1m.to_table(\n    nearest={\n        \"column\": \"vector\",\n        \"q\": samples[0],\n        \"k\": 10,\n        \"nprobes\": 10,\n        \"refine_factor\": 5,\n    }\n).to_pandas()\n</code></pre> <p>Parameter Explanation: - <code>q</code> =&gt; sample vector - <code>k</code> =&gt; how many neighbors to return - <code>nprobes</code> =&gt; how many partitions (in the coarse quantizer) to probe - <code>refine_factor</code> =&gt; controls \"re-ranking\". If k=10 and refine_factor=5 then retrieve 50 nearest neighbors by ANN and re-sort using actual distances then return top 10. This improves recall without sacrificing performance too much</p> <p>Memory Usage</p> <p>The latencies above include file I/O as Lance currently doesn't hold anything in memory. Along with index building speed, creating a purely in-memory version of the dataset would make the biggest impact on performance.</p>"},{"location":"quickstart/vector-search/#combine-features-and-vectors","title":"Combine Features and Vectors","text":"<p>You can add metadata columns to your vector dataset and query both vectors and features together in a single operation.</p> <p>In real-life situations, users have other feature or metadata columns that need to be stored and fetched together. If you're managing data and the index separately, you have to do a bunch of annoying plumbing to put stuff together. </p> <p>With Lance it's a single call:</p> <pre><code>tbl = sift1m.to_table()\ntbl = tbl.append_column(\"item_id\", pa.array(range(len(tbl))))\ntbl = tbl.append_column(\"revenue\", pa.array((np.random.randn(len(tbl))+5)*1000))\n</code></pre> <p>You can then query both vectors and metadata together:</p> <pre><code># Get vectors and metadata together\nresult = sift1m.to_table(\n    columns=[\"item_id\", \"revenue\"],\n    nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10}\n)\nprint(result.to_pandas())\n</code></pre>"},{"location":"quickstart/vector-search/#next-steps","title":"Next Steps","text":"<p>You should check out Versioning Your Datasets with Lance. We'll show you how to version your vector datasets and track changes over time.</p>"},{"location":"quickstart/versioning/","title":"Versioning Your Datasets with Lance","text":"<p>Lance supports versioning natively, allowing you to track changes over time. </p> <p>In this tutorial, you'll learn how to append new data to existing datasets while preserving historical versions and access specific versions using version numbers or meaningful tags. You'll also understand how to implement proper data governance practices with Lance's native versioning capabilities.</p>"},{"location":"quickstart/versioning/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/versioning/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, you should import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/versioning/#append-new-data-to-your-dataset","title":"Append New Data to Your Dataset","text":"<p>You can add new rows to your existing dataset, creating a new version while preserving the original data. Here is how to append rows:</p> <pre><code>df = pd.DataFrame({\"a\": [10]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"append\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#overwrite-your-dataset","title":"Overwrite Your Dataset","text":"<p>You can completely replace your dataset with new data, creating a new version while keeping the old version accessible.</p> <p>Here is how to overwrite the data and create a new version:</p> <pre><code>df = pd.DataFrame({\"a\": [50, 100]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"overwrite\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#access-previous-dataset-versions","title":"Access Previous Dataset Versions","text":"<p>You can also check what versions are available and then access specific versions of your dataset.</p> <p>List all versions of a dataset with this request:</p> <pre><code>dataset.versions()\n</code></pre> <p>You can also access any available version:</p> <pre><code># Version 1\nlance.dataset('/tmp/test.lance', version=1).to_table().to_pandas()\n\n# Version 2\nlance.dataset('/tmp/test.lance', version=2).to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#tag-your-important-versions","title":"Tag Your Important Versions","text":"<p>Create named tags for important versions, making it easier to reference specific versions by meaningful names. To create tags for relevant versions, do this:</p> <pre><code>dataset.tags.create(\"stable\", 2)\ndataset.tags.create(\"nightly\", 3)\ndataset.tags.list()\n</code></pre> <p>Tags can be checked out like versions:</p> <pre><code>lance.dataset('/tmp/test.lance', version=\"stable\").to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered dataset versioning with Lance, check out Vector Indexing and Vector Search With Lance. You can learn how to build high-performance vector search capabilities on top of your Lance tables.</p> <p>This will teach you how to build fast, scalable search capabilities for your versioned datasets. </p>"}]}